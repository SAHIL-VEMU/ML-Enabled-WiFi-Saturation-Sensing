{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieGxGR5ap3IJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjSE_ZtGp3Im"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"dataset_new.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9zDoM1rp3Io",
        "outputId": "ed982ffe-242d-4f40-8f42-465b9988f468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0       1       2       3       4       5       6       7       8   \\\n",
              "0  0.0065  0.0130  0.0195  0.0260  0.0325  0.0390  0.0455  0.0520  0.0585   \n",
              "1  0.0263  0.0526  0.0789  0.1052  0.1316  0.1579  0.1842  0.2105  0.2368   \n",
              "2  0.0176  0.0352  0.0528  0.0704  0.0881  0.1057  0.1233  0.1409  0.1585   \n",
              "3  0.0222  0.0444  0.0666  0.0888  0.1110  0.1332  0.1554  0.1776  0.1998   \n",
              "4  0.0189  0.0379  0.0568  0.0758  0.0947  0.1137  0.1326  0.1516  0.1705   \n",
              "\n",
              "       9   ...      45      46      47      48      49      50      51  \\\n",
              "0  0.0650  ...  0.0316  0.0368  0.0312  0.0000  0.0316  0.0364  0.0304   \n",
              "1  0.2631  ...  0.0316  0.0368  0.0312  0.0000  0.0316  0.0364  0.0304   \n",
              "2  0.1761  ...  0.0316  0.0368  0.0312  0.0312  0.0316  0.0299  0.0304   \n",
              "3  0.2220  ...  0.0316  0.0307  0.0312  0.0000  0.0360  0.0364  0.0304   \n",
              "4  0.1895  ...  0.0316  0.0368  0.0312  0.0000  0.0316  0.0364  0.0304   \n",
              "\n",
              "       52      53  54  \n",
              "0  0.0604  0.9960   1  \n",
              "1  0.0604  0.9960   1  \n",
              "2  0.0604  0.9957   1  \n",
              "3  0.0604  0.9900   1  \n",
              "4  0.0604  0.9862   1  \n",
              "\n",
              "[5 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-41903843-47c0-42fe-aa83-4c51e7f135b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0325</td>\n",
              "      <td>0.0390</td>\n",
              "      <td>0.0455</td>\n",
              "      <td>0.0520</td>\n",
              "      <td>0.0585</td>\n",
              "      <td>0.0650</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9960</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0263</td>\n",
              "      <td>0.0526</td>\n",
              "      <td>0.0789</td>\n",
              "      <td>0.1052</td>\n",
              "      <td>0.1316</td>\n",
              "      <td>0.1579</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.2105</td>\n",
              "      <td>0.2368</td>\n",
              "      <td>0.2631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9960</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0704</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>0.1409</td>\n",
              "      <td>0.1585</td>\n",
              "      <td>0.1761</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0299</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0888</td>\n",
              "      <td>0.1110</td>\n",
              "      <td>0.1332</td>\n",
              "      <td>0.1554</td>\n",
              "      <td>0.1776</td>\n",
              "      <td>0.1998</td>\n",
              "      <td>0.2220</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0307</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9900</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0189</td>\n",
              "      <td>0.0379</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.0947</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.1326</td>\n",
              "      <td>0.1516</td>\n",
              "      <td>0.1705</td>\n",
              "      <td>0.1895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9862</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41903843-47c0-42fe-aa83-4c51e7f135b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-29495569-e1f5-4a26-97e4-872493997e3a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29495569-e1f5-4a26-97e4-872493997e3a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-29495569-e1f5-4a26-97e4-872493997e3a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41903843-47c0-42fe-aa83-4c51e7f135b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41903843-47c0-42fe-aa83-4c51e7f135b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ee9ZqOp3JM",
        "outputId": "d9b12ac8-a070-4661-8171-25ca0015d72d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DSvmYRTp3Jc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten, Conv2D, BatchNormalization, MaxPooling2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py1LcOr1p3Jv"
      },
      "outputs": [],
      "source": [
        "def normalize_data(data):\n",
        "    data = np.transpose(data)\n",
        "    scalar = MinMaxScaler((0,1))\n",
        "    scalar.fit(data)\n",
        "    data = scalar.transform(data)\n",
        "    output = np.transpose(data)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-ZwXvpBp3Jz"
      },
      "outputs": [],
      "source": [
        "classes = ['Low_traffic', 'High_traffic']\n",
        "num_classes = len(classes)\n",
        "in_dim = [2, 13]\n",
        "X = df.iloc[:,0:26].to_numpy()\n",
        "Y = df.iloc[:,26:52].to_numpy()\n",
        "IPI = df.iloc[:,52].to_numpy()\n",
        "col = df.iloc[:,53].to_numpy()\n",
        "Label = df.iloc[:,54].to_numpy()\n",
        "Label_cat = tf.keras.utils.to_categorical(Label,num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOxRfcugp3J1",
        "outputId": "c87d128e-0504-42e0-893f-c056a81b7070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZBX6ZIgp3J2",
        "outputId": "ed605d0c-c022-4291-ed0e-3f3e3e6e62fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.    , 0.    , 0.4992, 0.002 , 0.    , 0.0012, 0.0368, 0.0404,\n",
              "       0.    , 0.0312, 0.0284, 0.    , 0.0296, 0.0368, 0.0344, 0.    ,\n",
              "       0.0312, 0.0304, 0.    , 0.0316, 0.0368, 0.0312, 0.    , 0.0316,\n",
              "       0.0364, 0.0304])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F0jZOUQp3J2",
        "outputId": "c94d0746-6562-42b5-a742-bbe07887e6c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0604"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "IPI[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_atpFtbyp3J3",
        "outputId": "8ca75868-b18d-496e-ddb7-80b87bc83e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.86889e+05, 2.37100e+04, 5.19300e+03, 2.31100e+03, 1.10400e+03,\n",
              "        4.63000e+02, 8.00000e+01, 7.00000e+01, 8.70000e+01, 9.30000e+01]),\n",
              " array([2.300000e-03, 4.910370e+00, 9.818440e+00, 1.472651e+01,\n",
              "        1.963458e+01, 2.454265e+01, 2.945072e+01, 3.435879e+01,\n",
              "        3.926686e+01, 4.417493e+01, 4.908300e+01]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.hist(np.array(X).flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP5tZa9Ip3KY"
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed=int(time.time()))\n",
        "n_examples = X.shape[0]\n",
        "n_train = n_examples * 0.7\n",
        "train_idx = np.random.choice(range(0,n_examples), size=int(n_train), replace=False)\n",
        "test_idx = list(set(range(0,n_examples))-set(train_idx))\n",
        "X_train = X[train_idx]\n",
        "X_test = X[test_idx]\n",
        "Y_train = Y[train_idx]\n",
        "Y_test = Y[test_idx]\n",
        "IPI_train = IPI[train_idx]\n",
        "IPI_test = IPI[test_idx]\n",
        "col_train = col[train_idx]\n",
        "col_test = col[test_idx]\n",
        "Label_train = Label_cat[train_idx]\n",
        "Label_test = Label_cat[test_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zztNcXjp3Kb"
      },
      "outputs": [],
      "source": [
        "def get_model(X_shape, Y_shape, IPI_shape, col_shape, classes, verbose):\n",
        "    dr = 0.30\n",
        "    model_X = tf.keras.models.Sequential()\n",
        "    model_X.add(layers.Reshape((X_shape, 1), input_shape=(X_shape, ) ))\n",
        "    model_X.add(layers.Conv1D(32, 3, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.BatchNormalization())\n",
        "\n",
        "    model_X.add(layers.Conv1D(16, 2, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.Flatten())\n",
        "    model_X.add(layers.Dense(64, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.Dropout(dr))\n",
        "    model_X.add(layers.Dense(32, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.Dropout(dr))\n",
        "\n",
        "    model_Y = tf.keras.models.Sequential()\n",
        "    model_Y.add(layers.Reshape((Y_shape, 1), input_shape=(Y_shape, ) ))\n",
        "    model_Y.add(layers.Conv1D(256, 3, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01) ))\n",
        "    model_Y.add(layers.BatchNormalization())\n",
        "\n",
        "    model_Y.add(layers.Conv1D(128, 2, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_Y.add(layers.Flatten())\n",
        "    model_Y.add(layers.Dropout(dr))\n",
        "    model_Y.add(layers.Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01) ))\n",
        "    model_Y.add(layers.Dropout(dr))\n",
        "    model_Y.add(layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_Y.add(layers.Dropout(dr))\n",
        "\n",
        "    model_IPI = tf.keras.models.Sequential()\n",
        "    model_IPI.add(layers.Dense(IPI_shape, activation=tf.nn.relu, input_shape=(IPI_shape, )))\n",
        "\n",
        "    model_IPI.add(layers.Dense(64, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_IPI.add(layers.Dropout(dr))\n",
        "    model_IPI.add(layers.Dense(32, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_IPI.add(layers.Dropout(dr))\n",
        "\n",
        "    model_col = tf.keras.models.Sequential()\n",
        "    model_col.add(layers.Dense(col_shape, activation=tf.nn.relu, input_shape=(col_shape, )))\n",
        "\n",
        "    model_col.add(layers.Dense(64, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_col.add(layers.Dropout(dr))\n",
        "    model_col.add(layers.Dense(32, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_col.add(layers.Dropout(dr))\n",
        "\n",
        "    merged = tf.keras.layers.Concatenate(axis=1)([model_X.output, model_Y.output, model_IPI.output, model_col.output])\n",
        "    merged = layers.Dense(256,activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(merged)\n",
        "    merged = layers.Dropout(dr)(merged)\n",
        "    merged = layers.Dense(128,activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(merged)\n",
        "    merged = layers.Dropout(dr)(merged)\n",
        "    merged = layers.Dense(len(classes), activation=tf.nn.softmax, name='class_output')(merged)\n",
        "\n",
        "    model = tf.keras.models.Model([model_X.input,model_Y.input, model_IPI.input, model_col.input], merged)\n",
        "\n",
        "    if(verbose):\n",
        "        from tensorflow.keras.utils import plot_model\n",
        "        from IPython.display import Image\n",
        "        plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "    print(\"--Model created\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFLYK5IUp3K-",
        "outputId": "9837a794-6813-4e49-c251-292c65f6f4f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Model created\n"
          ]
        }
      ],
      "source": [
        "model = get_model(X.shape[1], Y.shape[1], 1, 1, classes, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ9BUx1Sp3K_",
        "outputId": "2936cb7a-aa17-4a47-b195-592403b2c115",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " reshape_1_input (InputLayer)   [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " reshape_input (InputLayer)     [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 26, 1)        0           ['reshape_1_input[0][0]']        \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 26, 1)        0           ['reshape_input[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 24, 256)      1024        ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 24, 32)       128         ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 24, 256)     1024        ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 24, 32)      128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 23, 128)      65664       ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 23, 16)       1040        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 2944)         0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dense_4_input (InputLayer)     [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_7_input (InputLayer)     [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 368)          0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 2944)         0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            2           ['dense_4_input[0][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1)            2           ['dense_7_input[0][0]']          \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           23616       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          753920      ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           128         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 64)           128         ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 64)           0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           2080        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128)          32896       ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 32)           2080        ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 32)           2080        ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 32)           0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 32)           0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 224)          0           ['dropout_1[0][0]',              \n",
            "                                                                  'dropout_4[0][0]',              \n",
            "                                                                  'dropout_6[0][0]',              \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 256)          57600       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 256)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 128)          32896       ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 128)          0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " class_output (Dense)           (None, 2)            258         ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 976,694\n",
            "Trainable params: 976,118\n",
            "Non-trainable params: 576\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZGLbPEEp3LB",
        "outputId": "019a1bba-8de5-44d5-f45c-b8be98675c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss= 'categorical_crossentropy', metrics=['accuracy']  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6arPw7LSp3LG",
        "outputId": "a61e493e-3f1c-43f8-8431-b153275c5422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5000\n",
            "\n",
            "Epoch 1: loss improved from inf to 1.07186, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 47s - loss: 1.0719 - accuracy: 0.9361 - val_loss: 0.2558 - val_accuracy: 0.9530 - lr: 0.0010 - 47s/epoch - 53ms/step\n",
            "Epoch 2/5000\n",
            "\n",
            "Epoch 2: loss improved from 1.07186 to 0.25574, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.2557 - accuracy: 0.9512 - val_loss: 0.2154 - val_accuracy: 0.9632 - lr: 0.0010 - 27s/epoch - 30ms/step\n",
            "Epoch 3/5000\n",
            "\n",
            "Epoch 3: loss improved from 0.25574 to 0.22894, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.2289 - accuracy: 0.9544 - val_loss: 0.2251 - val_accuracy: 0.9528 - lr: 0.0010 - 27s/epoch - 30ms/step\n",
            "Epoch 4/5000\n",
            "\n",
            "Epoch 4: loss improved from 0.22894 to 0.21807, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.2181 - accuracy: 0.9552 - val_loss: 0.1837 - val_accuracy: 0.9670 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 5/5000\n",
            "\n",
            "Epoch 5: loss improved from 0.21807 to 0.21247, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 29s - loss: 0.2125 - accuracy: 0.9564 - val_loss: 0.1970 - val_accuracy: 0.9633 - lr: 0.0010 - 29s/epoch - 34ms/step\n",
            "Epoch 6/5000\n",
            "\n",
            "Epoch 6: loss improved from 0.21247 to 0.20924, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 30s - loss: 0.2092 - accuracy: 0.9579 - val_loss: 0.1688 - val_accuracy: 0.9730 - lr: 0.0010 - 30s/epoch - 35ms/step\n",
            "Epoch 7/5000\n",
            "\n",
            "Epoch 7: loss improved from 0.20924 to 0.19895, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1990 - accuracy: 0.9609 - val_loss: 0.1782 - val_accuracy: 0.9700 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 8/5000\n",
            "\n",
            "Epoch 8: loss improved from 0.19895 to 0.19506, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1951 - accuracy: 0.9631 - val_loss: 0.1593 - val_accuracy: 0.9750 - lr: 0.0010 - 26s/epoch - 30ms/step\n",
            "Epoch 9/5000\n",
            "\n",
            "Epoch 9: loss did not improve from 0.19506\n",
            "875/875 - 29s - loss: 0.1963 - accuracy: 0.9624 - val_loss: 0.1845 - val_accuracy: 0.9633 - lr: 0.0010 - 29s/epoch - 33ms/step\n",
            "Epoch 10/5000\n",
            "\n",
            "Epoch 10: loss improved from 0.19506 to 0.18786, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1879 - accuracy: 0.9630 - val_loss: 0.1649 - val_accuracy: 0.9735 - lr: 0.0010 - 26s/epoch - 29ms/step\n",
            "Epoch 11/5000\n",
            "\n",
            "Epoch 11: loss improved from 0.18786 to 0.18542, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1854 - accuracy: 0.9640 - val_loss: 0.1939 - val_accuracy: 0.9587 - lr: 0.0010 - 26s/epoch - 30ms/step\n",
            "Epoch 12/5000\n",
            "\n",
            "Epoch 12: loss improved from 0.18542 to 0.18514, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1851 - accuracy: 0.9636 - val_loss: 0.1658 - val_accuracy: 0.9727 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 13/5000\n",
            "\n",
            "Epoch 13: loss did not improve from 0.18514\n",
            "875/875 - 27s - loss: 0.1866 - accuracy: 0.9635 - val_loss: 0.1554 - val_accuracy: 0.9765 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 14/5000\n",
            "\n",
            "Epoch 14: loss improved from 0.18514 to 0.18016, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1802 - accuracy: 0.9654 - val_loss: 0.2290 - val_accuracy: 0.9465 - lr: 0.0010 - 28s/epoch - 33ms/step\n",
            "Epoch 15/5000\n",
            "\n",
            "Epoch 15: loss did not improve from 0.18016\n",
            "875/875 - 28s - loss: 0.1840 - accuracy: 0.9643 - val_loss: 0.1478 - val_accuracy: 0.9772 - lr: 0.0010 - 28s/epoch - 32ms/step\n",
            "Epoch 16/5000\n",
            "\n",
            "Epoch 16: loss did not improve from 0.18016\n",
            "875/875 - 25s - loss: 0.1806 - accuracy: 0.9654 - val_loss: 0.2459 - val_accuracy: 0.9347 - lr: 0.0010 - 25s/epoch - 28ms/step\n",
            "Epoch 17/5000\n",
            "\n",
            "Epoch 17: loss did not improve from 0.18016\n",
            "875/875 - 26s - loss: 0.1811 - accuracy: 0.9664 - val_loss: 0.1579 - val_accuracy: 0.9760 - lr: 0.0010 - 26s/epoch - 30ms/step\n",
            "Epoch 18/5000\n",
            "\n",
            "Epoch 18: loss improved from 0.18016 to 0.17804, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1780 - accuracy: 0.9664 - val_loss: 0.1630 - val_accuracy: 0.9747 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 19/5000\n",
            "\n",
            "Epoch 19: loss improved from 0.17804 to 0.17687, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1769 - accuracy: 0.9672 - val_loss: 0.1450 - val_accuracy: 0.9765 - lr: 0.0010 - 28s/epoch - 32ms/step\n",
            "Epoch 20/5000\n",
            "\n",
            "Epoch 20: loss improved from 0.17687 to 0.17185, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 29s - loss: 0.1718 - accuracy: 0.9656 - val_loss: 0.1436 - val_accuracy: 0.9795 - lr: 0.0010 - 29s/epoch - 33ms/step\n",
            "Epoch 21/5000\n",
            "\n",
            "Epoch 21: loss did not improve from 0.17185\n",
            "875/875 - 27s - loss: 0.1730 - accuracy: 0.9662 - val_loss: 0.1532 - val_accuracy: 0.9780 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 22/5000\n",
            "\n",
            "Epoch 22: loss did not improve from 0.17185\n",
            "875/875 - 26s - loss: 0.1729 - accuracy: 0.9674 - val_loss: 0.1655 - val_accuracy: 0.9682 - lr: 0.0010 - 26s/epoch - 29ms/step\n",
            "Epoch 23/5000\n",
            "\n",
            "Epoch 23: loss did not improve from 0.17185\n",
            "875/875 - 27s - loss: 0.1729 - accuracy: 0.9664 - val_loss: 0.2410 - val_accuracy: 0.9482 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 24/5000\n",
            "\n",
            "Epoch 24: loss improved from 0.17185 to 0.17053, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1705 - accuracy: 0.9675 - val_loss: 0.1401 - val_accuracy: 0.9793 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 25/5000\n",
            "\n",
            "Epoch 25: loss did not improve from 0.17053\n",
            "875/875 - 28s - loss: 0.1758 - accuracy: 0.9665 - val_loss: 0.2087 - val_accuracy: 0.9488 - lr: 0.0010 - 28s/epoch - 32ms/step\n",
            "Epoch 26/5000\n",
            "\n",
            "Epoch 26: loss did not improve from 0.17053\n",
            "875/875 - 28s - loss: 0.1715 - accuracy: 0.9659 - val_loss: 0.1553 - val_accuracy: 0.9768 - lr: 0.0010 - 28s/epoch - 31ms/step\n",
            "Epoch 27/5000\n",
            "\n",
            "Epoch 27: loss improved from 0.17053 to 0.16814, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1681 - accuracy: 0.9691 - val_loss: 0.1479 - val_accuracy: 0.9733 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 28/5000\n",
            "\n",
            "Epoch 28: loss improved from 0.16814 to 0.16598, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1660 - accuracy: 0.9681 - val_loss: 0.1493 - val_accuracy: 0.9747 - lr: 0.0010 - 28s/epoch - 32ms/step\n",
            "Epoch 29/5000\n",
            "\n",
            "Epoch 29: loss did not improve from 0.16598\n",
            "875/875 - 29s - loss: 0.1671 - accuracy: 0.9677 - val_loss: 0.1351 - val_accuracy: 0.9770 - lr: 0.0010 - 29s/epoch - 33ms/step\n",
            "Epoch 30/5000\n",
            "\n",
            "Epoch 30: loss did not improve from 0.16598\n",
            "875/875 - 28s - loss: 0.1698 - accuracy: 0.9664 - val_loss: 0.1925 - val_accuracy: 0.9583 - lr: 0.0010 - 28s/epoch - 32ms/step\n",
            "Epoch 31/5000\n",
            "\n",
            "Epoch 31: loss improved from 0.16598 to 0.16143, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1614 - accuracy: 0.9668 - val_loss: 0.1951 - val_accuracy: 0.9405 - lr: 0.0010 - 26s/epoch - 30ms/step\n",
            "Epoch 32/5000\n",
            "\n",
            "Epoch 32: loss did not improve from 0.16143\n",
            "875/875 - 28s - loss: 0.1636 - accuracy: 0.9670 - val_loss: 0.1489 - val_accuracy: 0.9780 - lr: 0.0010 - 28s/epoch - 32ms/step\n",
            "Epoch 33/5000\n",
            "\n",
            "Epoch 33: loss did not improve from 0.16143\n",
            "875/875 - 27s - loss: 0.1672 - accuracy: 0.9691 - val_loss: 0.1556 - val_accuracy: 0.9730 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 34/5000\n",
            "\n",
            "Epoch 34: loss did not improve from 0.16143\n",
            "875/875 - 26s - loss: 0.1747 - accuracy: 0.9676 - val_loss: 0.1504 - val_accuracy: 0.9730 - lr: 0.0010 - 26s/epoch - 30ms/step\n",
            "Epoch 35/5000\n",
            "\n",
            "Epoch 35: loss did not improve from 0.16143\n",
            "875/875 - 27s - loss: 0.1646 - accuracy: 0.9681 - val_loss: 0.1425 - val_accuracy: 0.9782 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 36/5000\n",
            "\n",
            "Epoch 36: loss did not improve from 0.16143\n",
            "875/875 - 27s - loss: 0.1623 - accuracy: 0.9675 - val_loss: 0.1389 - val_accuracy: 0.9777 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 37/5000\n",
            "\n",
            "Epoch 37: loss did not improve from 0.16143\n",
            "875/875 - 27s - loss: 0.1684 - accuracy: 0.9654 - val_loss: 0.1737 - val_accuracy: 0.9610 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 38/5000\n",
            "\n",
            "Epoch 38: loss did not improve from 0.16143\n",
            "875/875 - 27s - loss: 0.1615 - accuracy: 0.9675 - val_loss: 0.3093 - val_accuracy: 0.9377 - lr: 0.0010 - 27s/epoch - 31ms/step\n",
            "Epoch 39/5000\n",
            "\n",
            "Epoch 39: loss did not improve from 0.16143\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "875/875 - 26s - loss: 0.1634 - accuracy: 0.9674 - val_loss: 0.1366 - val_accuracy: 0.9808 - lr: 0.0010 - 26s/epoch - 29ms/step\n",
            "Epoch 40/5000\n",
            "\n",
            "Epoch 40: loss improved from 0.16143 to 0.14658, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1466 - accuracy: 0.9735 - val_loss: 0.1235 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 41/5000\n",
            "\n",
            "Epoch 41: loss improved from 0.14658 to 0.14225, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1422 - accuracy: 0.9743 - val_loss: 0.1224 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 42/5000\n",
            "\n",
            "Epoch 42: loss improved from 0.14225 to 0.13723, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1372 - accuracy: 0.9735 - val_loss: 0.1170 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 43/5000\n",
            "\n",
            "Epoch 43: loss did not improve from 0.13723\n",
            "875/875 - 28s - loss: 0.1407 - accuracy: 0.9711 - val_loss: 0.1156 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 44/5000\n",
            "\n",
            "Epoch 44: loss improved from 0.13723 to 0.13612, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1361 - accuracy: 0.9739 - val_loss: 0.1153 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 45/5000\n",
            "\n",
            "Epoch 45: loss improved from 0.13612 to 0.13573, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1357 - accuracy: 0.9720 - val_loss: 0.1132 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 46/5000\n",
            "\n",
            "Epoch 46: loss improved from 0.13573 to 0.13448, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1345 - accuracy: 0.9727 - val_loss: 0.1123 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 47/5000\n",
            "\n",
            "Epoch 47: loss improved from 0.13448 to 0.13245, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1325 - accuracy: 0.9720 - val_loss: 0.1129 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 48/5000\n",
            "\n",
            "Epoch 48: loss did not improve from 0.13245\n",
            "875/875 - 28s - loss: 0.1343 - accuracy: 0.9725 - val_loss: 0.1112 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 49/5000\n",
            "\n",
            "Epoch 49: loss improved from 0.13245 to 0.13071, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1307 - accuracy: 0.9735 - val_loss: 0.1104 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 50/5000\n",
            "\n",
            "Epoch 50: loss did not improve from 0.13071\n",
            "875/875 - 28s - loss: 0.1353 - accuracy: 0.9712 - val_loss: 0.1090 - val_accuracy: 0.9813 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 51/5000\n",
            "\n",
            "Epoch 51: loss did not improve from 0.13071\n",
            "875/875 - 27s - loss: 0.1327 - accuracy: 0.9719 - val_loss: 0.1102 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 52/5000\n",
            "\n",
            "Epoch 52: loss did not improve from 0.13071\n",
            "875/875 - 28s - loss: 0.1344 - accuracy: 0.9727 - val_loss: 0.1162 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 53/5000\n",
            "\n",
            "Epoch 53: loss improved from 0.13071 to 0.13070, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1307 - accuracy: 0.9727 - val_loss: 0.1071 - val_accuracy: 0.9815 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 54/5000\n",
            "\n",
            "Epoch 54: loss did not improve from 0.13070\n",
            "875/875 - 28s - loss: 0.1327 - accuracy: 0.9721 - val_loss: 0.1085 - val_accuracy: 0.9802 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 55/5000\n",
            "\n",
            "Epoch 55: loss improved from 0.13070 to 0.12881, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1288 - accuracy: 0.9734 - val_loss: 0.1133 - val_accuracy: 0.9798 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 56/5000\n",
            "\n",
            "Epoch 56: loss did not improve from 0.12881\n",
            "875/875 - 28s - loss: 0.1309 - accuracy: 0.9730 - val_loss: 0.1110 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 57/5000\n",
            "\n",
            "Epoch 57: loss did not improve from 0.12881\n",
            "875/875 - 28s - loss: 0.1349 - accuracy: 0.9714 - val_loss: 0.1150 - val_accuracy: 0.9770 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 58/5000\n",
            "\n",
            "Epoch 58: loss did not improve from 0.12881\n",
            "875/875 - 31s - loss: 0.1305 - accuracy: 0.9728 - val_loss: 0.1083 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 31s/epoch - 35ms/step\n",
            "Epoch 59/5000\n",
            "\n",
            "Epoch 59: loss improved from 0.12881 to 0.12854, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1285 - accuracy: 0.9725 - val_loss: 0.1062 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 60/5000\n",
            "\n",
            "Epoch 60: loss improved from 0.12854 to 0.12853, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1285 - accuracy: 0.9734 - val_loss: 0.1065 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 28s/epoch - 33ms/step\n",
            "Epoch 61/5000\n",
            "\n",
            "Epoch 61: loss did not improve from 0.12853\n",
            "875/875 - 28s - loss: 0.1300 - accuracy: 0.9715 - val_loss: 0.1059 - val_accuracy: 0.9803 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 62/5000\n",
            "\n",
            "Epoch 62: loss did not improve from 0.12853\n",
            "875/875 - 28s - loss: 0.1292 - accuracy: 0.9735 - val_loss: 0.1049 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 63/5000\n",
            "\n",
            "Epoch 63: loss did not improve from 0.12853\n",
            "875/875 - 29s - loss: 0.1320 - accuracy: 0.9714 - val_loss: 0.1068 - val_accuracy: 0.9802 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 64/5000\n",
            "\n",
            "Epoch 64: loss did not improve from 0.12853\n",
            "875/875 - 29s - loss: 0.1300 - accuracy: 0.9733 - val_loss: 0.1063 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 65/5000\n",
            "\n",
            "Epoch 65: loss improved from 0.12853 to 0.12758, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1276 - accuracy: 0.9725 - val_loss: 0.1046 - val_accuracy: 0.9813 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 66/5000\n",
            "\n",
            "Epoch 66: loss did not improve from 0.12758\n",
            "875/875 - 26s - loss: 0.1292 - accuracy: 0.9721 - val_loss: 0.1082 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 67/5000\n",
            "\n",
            "Epoch 67: loss did not improve from 0.12758\n",
            "875/875 - 27s - loss: 0.1315 - accuracy: 0.9706 - val_loss: 0.1073 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 68/5000\n",
            "\n",
            "Epoch 68: loss did not improve from 0.12758\n",
            "875/875 - 28s - loss: 0.1315 - accuracy: 0.9716 - val_loss: 0.1063 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 69/5000\n",
            "\n",
            "Epoch 69: loss did not improve from 0.12758\n",
            "875/875 - 27s - loss: 0.1291 - accuracy: 0.9719 - val_loss: 0.1049 - val_accuracy: 0.9815 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 70/5000\n",
            "\n",
            "Epoch 70: loss did not improve from 0.12758\n",
            "875/875 - 27s - loss: 0.1319 - accuracy: 0.9699 - val_loss: 0.1067 - val_accuracy: 0.9782 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 71/5000\n",
            "\n",
            "Epoch 71: loss did not improve from 0.12758\n",
            "875/875 - 25s - loss: 0.1304 - accuracy: 0.9742 - val_loss: 0.1052 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 72/5000\n",
            "\n",
            "Epoch 72: loss improved from 0.12758 to 0.12634, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1263 - accuracy: 0.9721 - val_loss: 0.1046 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 73/5000\n",
            "\n",
            "Epoch 73: loss did not improve from 0.12634\n",
            "875/875 - 27s - loss: 0.1279 - accuracy: 0.9724 - val_loss: 0.1024 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 74/5000\n",
            "\n",
            "Epoch 74: loss did not improve from 0.12634\n",
            "875/875 - 29s - loss: 0.1287 - accuracy: 0.9724 - val_loss: 0.1029 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 75/5000\n",
            "\n",
            "Epoch 75: loss did not improve from 0.12634\n",
            "875/875 - 27s - loss: 0.1303 - accuracy: 0.9721 - val_loss: 0.1059 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 76/5000\n",
            "\n",
            "Epoch 76: loss did not improve from 0.12634\n",
            "875/875 - 25s - loss: 0.1286 - accuracy: 0.9716 - val_loss: 0.1056 - val_accuracy: 0.9790 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 77/5000\n",
            "\n",
            "Epoch 77: loss did not improve from 0.12634\n",
            "875/875 - 26s - loss: 0.1280 - accuracy: 0.9717 - val_loss: 0.1032 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 78/5000\n",
            "\n",
            "Epoch 78: loss improved from 0.12634 to 0.12568, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1257 - accuracy: 0.9730 - val_loss: 0.1050 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 79/5000\n",
            "\n",
            "Epoch 79: loss did not improve from 0.12568\n",
            "875/875 - 27s - loss: 0.1272 - accuracy: 0.9724 - val_loss: 0.1023 - val_accuracy: 0.9815 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 80/5000\n",
            "\n",
            "Epoch 80: loss did not improve from 0.12568\n",
            "875/875 - 26s - loss: 0.1299 - accuracy: 0.9700 - val_loss: 0.1066 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 81/5000\n",
            "\n",
            "Epoch 81: loss did not improve from 0.12568\n",
            "875/875 - 26s - loss: 0.1286 - accuracy: 0.9718 - val_loss: 0.1050 - val_accuracy: 0.9802 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 82/5000\n",
            "\n",
            "Epoch 82: loss did not improve from 0.12568\n",
            "875/875 - 28s - loss: 0.1281 - accuracy: 0.9709 - val_loss: 0.1034 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 28s/epoch - 31ms/step\n",
            "Epoch 83/5000\n",
            "\n",
            "Epoch 83: loss did not improve from 0.12568\n",
            "875/875 - 28s - loss: 0.1289 - accuracy: 0.9718 - val_loss: 0.1039 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 84/5000\n",
            "\n",
            "Epoch 84: loss did not improve from 0.12568\n",
            "875/875 - 27s - loss: 0.1279 - accuracy: 0.9722 - val_loss: 0.1136 - val_accuracy: 0.9755 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 85/5000\n",
            "\n",
            "Epoch 85: loss did not improve from 0.12568\n",
            "875/875 - 28s - loss: 0.1275 - accuracy: 0.9720 - val_loss: 0.1029 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 86/5000\n",
            "\n",
            "Epoch 86: loss improved from 0.12568 to 0.12384, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1238 - accuracy: 0.9722 - val_loss: 0.1046 - val_accuracy: 0.9790 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 87/5000\n",
            "\n",
            "Epoch 87: loss did not improve from 0.12384\n",
            "875/875 - 28s - loss: 0.1272 - accuracy: 0.9711 - val_loss: 0.1043 - val_accuracy: 0.9793 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 88/5000\n",
            "\n",
            "Epoch 88: loss did not improve from 0.12384\n",
            "875/875 - 29s - loss: 0.1250 - accuracy: 0.9719 - val_loss: 0.1048 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 89/5000\n",
            "\n",
            "Epoch 89: loss did not improve from 0.12384\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "875/875 - 29s - loss: 0.1288 - accuracy: 0.9726 - val_loss: 0.1041 - val_accuracy: 0.9803 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 90/5000\n",
            "\n",
            "Epoch 90: loss improved from 0.12384 to 0.12266, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 29s - loss: 0.1227 - accuracy: 0.9738 - val_loss: 0.1043 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 29s/epoch - 34ms/step\n",
            "Epoch 91/5000\n",
            "\n",
            "Epoch 91: loss did not improve from 0.12266\n",
            "875/875 - 28s - loss: 0.1251 - accuracy: 0.9726 - val_loss: 0.1003 - val_accuracy: 0.9818 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 92/5000\n",
            "\n",
            "Epoch 92: loss did not improve from 0.12266\n",
            "875/875 - 26s - loss: 0.1273 - accuracy: 0.9731 - val_loss: 0.1018 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 93/5000\n",
            "\n",
            "Epoch 93: loss did not improve from 0.12266\n",
            "875/875 - 27s - loss: 0.1254 - accuracy: 0.9730 - val_loss: 0.1019 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 94/5000\n",
            "\n",
            "Epoch 94: loss did not improve from 0.12266\n",
            "875/875 - 28s - loss: 0.1256 - accuracy: 0.9716 - val_loss: 0.1022 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 95/5000\n",
            "\n",
            "Epoch 95: loss did not improve from 0.12266\n",
            "875/875 - 27s - loss: 0.1274 - accuracy: 0.9707 - val_loss: 0.1014 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 96/5000\n",
            "\n",
            "Epoch 96: loss did not improve from 0.12266\n",
            "875/875 - 26s - loss: 0.1231 - accuracy: 0.9724 - val_loss: 0.1018 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 97/5000\n",
            "\n",
            "Epoch 97: loss did not improve from 0.12266\n",
            "875/875 - 26s - loss: 0.1244 - accuracy: 0.9731 - val_loss: 0.1031 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 98/5000\n",
            "\n",
            "Epoch 98: loss did not improve from 0.12266\n",
            "875/875 - 26s - loss: 0.1251 - accuracy: 0.9720 - val_loss: 0.1022 - val_accuracy: 0.9803 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 99/5000\n",
            "\n",
            "Epoch 99: loss did not improve from 0.12266\n",
            "875/875 - 28s - loss: 0.1268 - accuracy: 0.9725 - val_loss: 0.1074 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 100/5000\n",
            "\n",
            "Epoch 100: loss did not improve from 0.12266\n",
            "875/875 - 27s - loss: 0.1231 - accuracy: 0.9742 - val_loss: 0.1009 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 101/5000\n",
            "\n",
            "Epoch 101: loss did not improve from 0.12266\n",
            "875/875 - 27s - loss: 0.1255 - accuracy: 0.9719 - val_loss: 0.1054 - val_accuracy: 0.9785 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 102/5000\n",
            "\n",
            "Epoch 102: loss did not improve from 0.12266\n",
            "875/875 - 29s - loss: 0.1235 - accuracy: 0.9725 - val_loss: 0.1012 - val_accuracy: 0.9802 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 103/5000\n",
            "\n",
            "Epoch 103: loss did not improve from 0.12266\n",
            "875/875 - 27s - loss: 0.1271 - accuracy: 0.9723 - val_loss: 0.1018 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 104/5000\n",
            "\n",
            "Epoch 104: loss did not improve from 0.12266\n",
            "875/875 - 25s - loss: 0.1250 - accuracy: 0.9733 - val_loss: 0.1001 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 105/5000\n",
            "\n",
            "Epoch 105: loss improved from 0.12266 to 0.12265, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1227 - accuracy: 0.9737 - val_loss: 0.0999 - val_accuracy: 0.9803 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 106/5000\n",
            "\n",
            "Epoch 106: loss did not improve from 0.12265\n",
            "875/875 - 27s - loss: 0.1274 - accuracy: 0.9723 - val_loss: 0.1011 - val_accuracy: 0.9795 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 107/5000\n",
            "\n",
            "Epoch 107: loss did not improve from 0.12265\n",
            "875/875 - 27s - loss: 0.1268 - accuracy: 0.9718 - val_loss: 0.1007 - val_accuracy: 0.9787 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 108/5000\n",
            "\n",
            "Epoch 108: loss improved from 0.12265 to 0.12206, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1221 - accuracy: 0.9721 - val_loss: 0.1012 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 109/5000\n",
            "\n",
            "Epoch 109: loss did not improve from 0.12206\n",
            "875/875 - 26s - loss: 0.1264 - accuracy: 0.9721 - val_loss: 0.1010 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 110/5000\n",
            "\n",
            "Epoch 110: loss did not improve from 0.12206\n",
            "875/875 - 27s - loss: 0.1239 - accuracy: 0.9727 - val_loss: 0.1000 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 111/5000\n",
            "\n",
            "Epoch 111: loss did not improve from 0.12206\n",
            "875/875 - 28s - loss: 0.1249 - accuracy: 0.9726 - val_loss: 0.0995 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 112/5000\n",
            "\n",
            "Epoch 112: loss did not improve from 0.12206\n",
            "875/875 - 28s - loss: 0.1263 - accuracy: 0.9714 - val_loss: 0.1022 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 113/5000\n",
            "\n",
            "Epoch 113: loss did not improve from 0.12206\n",
            "875/875 - 27s - loss: 0.1258 - accuracy: 0.9731 - val_loss: 0.1003 - val_accuracy: 0.9798 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 114/5000\n",
            "\n",
            "Epoch 114: loss did not improve from 0.12206\n",
            "875/875 - 28s - loss: 0.1246 - accuracy: 0.9738 - val_loss: 0.1000 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 115/5000\n",
            "\n",
            "Epoch 115: loss did not improve from 0.12206\n",
            "875/875 - 27s - loss: 0.1258 - accuracy: 0.9711 - val_loss: 0.1003 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 116/5000\n",
            "\n",
            "Epoch 116: loss did not improve from 0.12206\n",
            "875/875 - 29s - loss: 0.1258 - accuracy: 0.9715 - val_loss: 0.1037 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 29s/epoch - 34ms/step\n",
            "Epoch 117/5000\n",
            "\n",
            "Epoch 117: loss did not improve from 0.12206\n",
            "875/875 - 25s - loss: 0.1235 - accuracy: 0.9729 - val_loss: 0.1013 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 118/5000\n",
            "\n",
            "Epoch 118: loss did not improve from 0.12206\n",
            "875/875 - 26s - loss: 0.1231 - accuracy: 0.9734 - val_loss: 0.1004 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 119/5000\n",
            "\n",
            "Epoch 119: loss did not improve from 0.12206\n",
            "875/875 - 26s - loss: 0.1257 - accuracy: 0.9711 - val_loss: 0.0993 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 120/5000\n",
            "\n",
            "Epoch 120: loss did not improve from 0.12206\n",
            "875/875 - 26s - loss: 0.1252 - accuracy: 0.9727 - val_loss: 0.1017 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 121/5000\n",
            "\n",
            "Epoch 121: loss did not improve from 0.12206\n",
            "875/875 - 28s - loss: 0.1245 - accuracy: 0.9714 - val_loss: 0.0988 - val_accuracy: 0.9810 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 122/5000\n",
            "\n",
            "Epoch 122: loss did not improve from 0.12206\n",
            "875/875 - 25s - loss: 0.1235 - accuracy: 0.9726 - val_loss: 0.0981 - val_accuracy: 0.9817 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 123/5000\n",
            "\n",
            "Epoch 123: loss did not improve from 0.12206\n",
            "875/875 - 26s - loss: 0.1231 - accuracy: 0.9734 - val_loss: 0.0981 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 124/5000\n",
            "\n",
            "Epoch 124: loss did not improve from 0.12206\n",
            "875/875 - 27s - loss: 0.1231 - accuracy: 0.9729 - val_loss: 0.0984 - val_accuracy: 0.9810 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 125/5000\n",
            "\n",
            "Epoch 125: loss did not improve from 0.12206\n",
            "875/875 - 27s - loss: 0.1251 - accuracy: 0.9717 - val_loss: 0.0996 - val_accuracy: 0.9798 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 126/5000\n",
            "\n",
            "Epoch 126: loss did not improve from 0.12206\n",
            "875/875 - 25s - loss: 0.1255 - accuracy: 0.9711 - val_loss: 0.0988 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 127/5000\n",
            "\n",
            "Epoch 127: loss improved from 0.12206 to 0.12106, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 26s - loss: 0.1211 - accuracy: 0.9739 - val_loss: 0.0977 - val_accuracy: 0.9810 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 128/5000\n",
            "\n",
            "Epoch 128: loss did not improve from 0.12106\n",
            "875/875 - 26s - loss: 0.1222 - accuracy: 0.9727 - val_loss: 0.0992 - val_accuracy: 0.9800 - lr: 1.0000e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 129/5000\n",
            "\n",
            "Epoch 129: loss did not improve from 0.12106\n",
            "875/875 - 27s - loss: 0.1233 - accuracy: 0.9722 - val_loss: 0.0994 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 130/5000\n",
            "\n",
            "Epoch 130: loss did not improve from 0.12106\n",
            "875/875 - 28s - loss: 0.1243 - accuracy: 0.9725 - val_loss: 0.1010 - val_accuracy: 0.9785 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 131/5000\n",
            "\n",
            "Epoch 131: loss did not improve from 0.12106\n",
            "875/875 - 27s - loss: 0.1224 - accuracy: 0.9716 - val_loss: 0.0975 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 132/5000\n",
            "\n",
            "Epoch 132: loss did not improve from 0.12106\n",
            "875/875 - 26s - loss: 0.1213 - accuracy: 0.9727 - val_loss: 0.0992 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 133/5000\n",
            "\n",
            "Epoch 133: loss did not improve from 0.12106\n",
            "875/875 - 27s - loss: 0.1238 - accuracy: 0.9719 - val_loss: 0.0977 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 134/5000\n",
            "\n",
            "Epoch 134: loss did not improve from 0.12106\n",
            "875/875 - 28s - loss: 0.1240 - accuracy: 0.9731 - val_loss: 0.1001 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 135/5000\n",
            "\n",
            "Epoch 135: loss did not improve from 0.12106\n",
            "875/875 - 27s - loss: 0.1257 - accuracy: 0.9720 - val_loss: 0.0979 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 136/5000\n",
            "\n",
            "Epoch 136: loss did not improve from 0.12106\n",
            "875/875 - 29s - loss: 0.1251 - accuracy: 0.9716 - val_loss: 0.0995 - val_accuracy: 0.9798 - lr: 1.0000e-04 - 29s/epoch - 33ms/step\n",
            "Epoch 137/5000\n",
            "\n",
            "Epoch 137: loss improved from 0.12106 to 0.12021, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 27s - loss: 0.1202 - accuracy: 0.9741 - val_loss: 0.1021 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 138/5000\n",
            "\n",
            "Epoch 138: loss did not improve from 0.12021\n",
            "875/875 - 25s - loss: 0.1238 - accuracy: 0.9730 - val_loss: 0.1010 - val_accuracy: 0.9797 - lr: 1.0000e-04 - 25s/epoch - 28ms/step\n",
            "Epoch 139/5000\n",
            "\n",
            "Epoch 139: loss did not improve from 0.12021\n",
            "875/875 - 26s - loss: 0.1244 - accuracy: 0.9724 - val_loss: 0.0993 - val_accuracy: 0.9792 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 140/5000\n",
            "\n",
            "Epoch 140: loss did not improve from 0.12021\n",
            "875/875 - 27s - loss: 0.1224 - accuracy: 0.9737 - val_loss: 0.0988 - val_accuracy: 0.9810 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 141/5000\n",
            "\n",
            "Epoch 141: loss did not improve from 0.12021\n",
            "875/875 - 26s - loss: 0.1228 - accuracy: 0.9736 - val_loss: 0.1047 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 142/5000\n",
            "\n",
            "Epoch 142: loss improved from 0.12021 to 0.11912, saving model to model_LTE_WiFi_coexistance_histogram+IPI+col.h5\n",
            "875/875 - 28s - loss: 0.1191 - accuracy: 0.9739 - val_loss: 0.0977 - val_accuracy: 0.9798 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 143/5000\n",
            "\n",
            "Epoch 143: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1226 - accuracy: 0.9724 - val_loss: 0.0980 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 144/5000\n",
            "\n",
            "Epoch 144: loss did not improve from 0.11912\n",
            "875/875 - 26s - loss: 0.1194 - accuracy: 0.9747 - val_loss: 0.0983 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 145/5000\n",
            "\n",
            "Epoch 145: loss did not improve from 0.11912\n",
            "875/875 - 26s - loss: 0.1224 - accuracy: 0.9728 - val_loss: 0.1015 - val_accuracy: 0.9793 - lr: 1.0000e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 146/5000\n",
            "\n",
            "Epoch 146: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1235 - accuracy: 0.9721 - val_loss: 0.0970 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 147/5000\n",
            "\n",
            "Epoch 147: loss did not improve from 0.11912\n",
            "875/875 - 28s - loss: 0.1200 - accuracy: 0.9741 - val_loss: 0.0981 - val_accuracy: 0.9793 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 148/5000\n",
            "\n",
            "Epoch 148: loss did not improve from 0.11912\n",
            "875/875 - 28s - loss: 0.1207 - accuracy: 0.9721 - val_loss: 0.0958 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 149/5000\n",
            "\n",
            "Epoch 149: loss did not improve from 0.11912\n",
            "875/875 - 24s - loss: 0.1206 - accuracy: 0.9724 - val_loss: 0.0980 - val_accuracy: 0.9807 - lr: 1.0000e-04 - 24s/epoch - 28ms/step\n",
            "Epoch 150/5000\n",
            "\n",
            "Epoch 150: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1213 - accuracy: 0.9737 - val_loss: 0.0969 - val_accuracy: 0.9790 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 151/5000\n",
            "\n",
            "Epoch 151: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1216 - accuracy: 0.9720 - val_loss: 0.1050 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 152/5000\n",
            "\n",
            "Epoch 152: loss did not improve from 0.11912\n",
            "875/875 - 30s - loss: 0.1246 - accuracy: 0.9708 - val_loss: 0.0968 - val_accuracy: 0.9805 - lr: 1.0000e-04 - 30s/epoch - 34ms/step\n",
            "Epoch 153/5000\n",
            "\n",
            "Epoch 153: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1237 - accuracy: 0.9720 - val_loss: 0.1074 - val_accuracy: 0.9755 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 154/5000\n",
            "\n",
            "Epoch 154: loss did not improve from 0.11912\n",
            "875/875 - 25s - loss: 0.1270 - accuracy: 0.9708 - val_loss: 0.0984 - val_accuracy: 0.9803 - lr: 1.0000e-04 - 25s/epoch - 29ms/step\n",
            "Epoch 155/5000\n",
            "\n",
            "Epoch 155: loss did not improve from 0.11912\n",
            "875/875 - 26s - loss: 0.1217 - accuracy: 0.9726 - val_loss: 0.1001 - val_accuracy: 0.9790 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 156/5000\n",
            "\n",
            "Epoch 156: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1216 - accuracy: 0.9709 - val_loss: 0.1005 - val_accuracy: 0.9793 - lr: 1.0000e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 157/5000\n",
            "\n",
            "Epoch 157: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1218 - accuracy: 0.9718 - val_loss: 0.1011 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 158/5000\n",
            "\n",
            "Epoch 158: loss did not improve from 0.11912\n",
            "875/875 - 28s - loss: 0.1248 - accuracy: 0.9716 - val_loss: 0.0971 - val_accuracy: 0.9813 - lr: 1.0000e-04 - 28s/epoch - 32ms/step\n",
            "Epoch 159/5000\n",
            "\n",
            "Epoch 159: loss did not improve from 0.11912\n",
            "875/875 - 24s - loss: 0.1195 - accuracy: 0.9732 - val_loss: 0.0970 - val_accuracy: 0.9812 - lr: 1.0000e-04 - 24s/epoch - 28ms/step\n",
            "Epoch 160/5000\n",
            "\n",
            "Epoch 160: loss did not improve from 0.11912\n",
            "875/875 - 26s - loss: 0.1215 - accuracy: 0.9716 - val_loss: 0.1012 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 161/5000\n",
            "\n",
            "Epoch 161: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1206 - accuracy: 0.9731 - val_loss: 0.0971 - val_accuracy: 0.9808 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 162/5000\n",
            "\n",
            "Epoch 162: loss did not improve from 0.11912\n",
            "875/875 - 27s - loss: 0.1221 - accuracy: 0.9724 - val_loss: 0.0982 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 27s/epoch - 31ms/step\n"
          ]
        }
      ],
      "source": [
        "savedir = 'model_LTE_WiFi_coexistance_histogram+IPI+col.h5'\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'loss', min_delta = 0, patience = 20, verbose = 0, mode = 'auto')\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(savedir, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_delta=0.00001, cooldown=0, min_lr=0.0001)\n",
        "nb_epoch = 5000\n",
        "batch_size = 16\n",
        "history = model.fit(x = [X_train, Y_train, np.transpose(IPI_train), np.transpose(col_train)], y = Label_train ,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=nb_epoch,\n",
        "                    validation_data = ([X_test, Y_test, np.transpose(IPI_test), np.transpose(col_test)], Label_test),\n",
        "                    shuffle=True,\n",
        "                    verbose=2,\n",
        "                    callbacks = [early_stop, checkpoint, reduce_lr_callback] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm9xx5yRp3Ld",
        "outputId": "df176664-e7f6-43e6-c0be-7e6cad761e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6bc2a3252f4a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msHaRcoPp3Lj",
        "outputId": "2aa62280-32e1-4f64-ac31-d70cb24cbc0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
          ]
        }
      ],
      "source": [
        "print(history.history.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYb86qEqp3Lk",
        "outputId": "e2f740a8-1d9f-4479-dd52-f372a77206b3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_XTSGBhp3Lm"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=45)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSt3rYE9p3Lm",
        "outputId": "116f3afe-0acd-4ec9-b22e-5cb1419504ed"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "X = [X_test, Y_test, np.transpose(IPI_test), np.transpose(col_test)]\n",
        "\n",
        "test_Y_hat = model.predict(X, batch_size=batch_size)\n",
        "conf = np.zeros([len(classes),len(classes)])\n",
        "confnorm1 = np.zeros([len(classes),len(classes)])\n",
        "for i in range(0,X_test.shape[0]):\n",
        "    j = list(Label_test[i,:]).index(1)\n",
        "    k = int(np.argmax(test_Y_hat[i,:]))\n",
        "    conf[j,k] = conf[j,k] + 1\n",
        "for i in range(0,len(classes)):\n",
        "    confnorm1[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
        "plot_confusion_matrix(confnorm1, labels=classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3WzwhaVp3Ls",
        "outputId": "c2f8c71a-e9e3-4e1a-cb8b-676643b88f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.98081454 0.01918546]\n",
            " [0.0227798  0.9772202 ]]\n"
          ]
        }
      ],
      "source": [
        "print(confnorm1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eDZKn9up3MG",
        "outputId": "b69b87b0-09ff-4531-d0fe-dc385d83440d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IPI.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahhCQ90_p3MI",
        "outputId": "6b028c99-f78d-4465-de3e-276ee3f4b9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5fe6826c6684>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIPI_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict([X_test, Y_test, np.transpose(IPI_test), np.transpose(col_test)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4LdoQzZp3MI"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh7wP5qMp3MJ"
      },
      "outputs": [],
      "source": [
        "Label_test = np.argmax(Label_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cqg5ksvp3MJ",
        "outputId": "a6886109-5ba8-4cae-d1ee-8f6ab3346528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.979\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(Label_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB5Tn_p_p3MK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6F0wsWsp3MP"
      },
      "outputs": [],
      "source": [
        "batch_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO-Imn0Np3MQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset_test.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVi672G1p3MR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c70c11b-69e4-478b-c53f-c84293b6ced1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0       1       2       3       4       5       6       7       8   \\\n",
              "0   0.0662  0.1324  0.1986  0.2648  0.3310  0.3972  0.4634  0.5296  0.5958   \n",
              "1   0.0662  0.1324  0.1986  0.2648  0.3310  0.3972  0.4634  0.5296  0.5958   \n",
              "2   0.2692  0.5384  0.8076  1.0768  1.3460  1.6152  1.8844  2.1536  2.4228   \n",
              "3   0.0380  0.0760  0.1140  0.1520  0.1900  0.2280  0.2660  0.3040  0.3420   \n",
              "4   0.0383  0.0766  0.1149  0.1532  0.1915  0.2298  0.2681  0.3064  0.3447   \n",
              "5   0.0373  0.0746  0.1119  0.1492  0.1865  0.2238  0.2611  0.2984  0.3357   \n",
              "6   0.0380  0.0760  0.1140  0.1520  0.1900  0.2280  0.2660  0.3040  0.3420   \n",
              "7   0.0376  0.0752  0.1128  0.1504  0.1880  0.2256  0.2632  0.3008  0.3384   \n",
              "8   0.0379  0.0758  0.1137  0.1516  0.1895  0.2274  0.2653  0.3032  0.3411   \n",
              "9   0.0376  0.0752  0.1128  0.1504  0.1880  0.2256  0.2632  0.3008  0.3384   \n",
              "10  0.0584  0.1168  0.1752  0.2336  0.2920  0.3504  0.4088  0.4672  0.5256   \n",
              "11  0.0584  0.1168  0.1752  0.2336  0.2920  0.3504  0.4088  0.4672  0.5256   \n",
              "12  0.0247  0.0494  0.0741  0.0988  0.1235  0.1482  0.1729  0.1976  0.2223   \n",
              "13  0.0146  0.0292  0.0438  0.0584  0.0730  0.0876  0.1022  0.1168  0.1314   \n",
              "14  0.0172  0.0344  0.0516  0.0688  0.0860  0.1032  0.1204  0.1376  0.1548   \n",
              "15  0.0363  0.0726  0.1089  0.1452  0.1815  0.2178  0.2541  0.2904  0.3267   \n",
              "16  0.0363  0.0726  0.1089  0.1452  0.1815  0.2178  0.2541  0.2904  0.3267   \n",
              "17  0.0363  0.0726  0.1089  0.1452  0.1815  0.2178  0.2541  0.2904  0.3267   \n",
              "18  0.0363  0.0726  0.1089  0.1452  0.1815  0.2178  0.2541  0.2904  0.3267   \n",
              "19  0.0088  0.0176  0.0264  0.0352  0.0440  0.0528  0.0616  0.0704  0.0792   \n",
              "20  0.0067  0.0134  0.0201  0.0268  0.0335  0.0402  0.0469  0.0536  0.0603   \n",
              "21  0.0071  0.0142  0.0213  0.0284  0.0355  0.0426  0.0497  0.0568  0.0639   \n",
              "22  0.0074  0.0148  0.0222  0.0296  0.0370  0.0444  0.0518  0.0592  0.0666   \n",
              "23  0.0067  0.0134  0.0201  0.0268  0.0335  0.0402  0.0469  0.0536  0.0603   \n",
              "24  0.0067  0.0134  0.0201  0.0268  0.0335  0.0402  0.0469  0.0536  0.0603   \n",
              "25  0.0071  0.0142  0.0213  0.0284  0.0355  0.0426  0.0497  0.0568  0.0639   \n",
              "26  0.0078  0.0156  0.0234  0.0312  0.0390  0.0468  0.0546  0.0624  0.0702   \n",
              "27  0.0067  0.0134  0.0201  0.0268  0.0335  0.0402  0.0469  0.0536  0.0603   \n",
              "28  0.0085  0.0170  0.0255  0.0340  0.0425  0.0510  0.0595  0.0680  0.0765   \n",
              "29  0.0085  0.0170  0.0255  0.0340  0.0425  0.0510  0.0595  0.0680  0.0765   \n",
              "30  0.0074  0.0148  0.0222  0.0296  0.0370  0.0444  0.0518  0.0592  0.0666   \n",
              "31  0.0080  0.0160  0.0240  0.0320  0.0400  0.0480  0.0560  0.0640  0.0720   \n",
              "32  0.0073  0.0146  0.0219  0.0292  0.0365  0.0438  0.0511  0.0584  0.0657   \n",
              "33  0.0081  0.0162  0.0243  0.0324  0.0405  0.0486  0.0567  0.0648  0.0729   \n",
              "34  0.0109  0.0218  0.0327  0.0436  0.0545  0.0654  0.0763  0.0872  0.0981   \n",
              "35  0.0122  0.0244  0.0366  0.0488  0.0610  0.0732  0.0854  0.0976  0.1098   \n",
              "36  0.0101  0.0202  0.0303  0.0404  0.0505  0.0606  0.0707  0.0808  0.0909   \n",
              "37  0.0111  0.0222  0.0333  0.0444  0.0555  0.0666  0.0777  0.0888  0.0999   \n",
              "38  0.0104  0.0208  0.0312  0.0416  0.0520  0.0624  0.0728  0.0832  0.0936   \n",
              "39  0.0118  0.0236  0.0354  0.0472  0.0590  0.0708  0.0826  0.0944  0.1062   \n",
              "40  0.0078  0.0156  0.0234  0.0312  0.0390  0.0468  0.0546  0.0624  0.0702   \n",
              "41  0.0074  0.0148  0.0222  0.0296  0.0370  0.0444  0.0518  0.0592  0.0666   \n",
              "42  0.0060  0.0120  0.0180  0.0240  0.0300  0.0360  0.0420  0.0480  0.0540   \n",
              "43  0.0072  0.0144  0.0216  0.0288  0.0360  0.0432  0.0504  0.0576  0.0648   \n",
              "44  0.0067  0.0134  0.0201  0.0268  0.0335  0.0402  0.0469  0.0536  0.0603   \n",
              "45  0.1557  0.3114  0.4671  0.6228  0.7785  0.9342  1.0899  1.2456  1.4013   \n",
              "46  0.0723  0.1446  0.2169  0.2892  0.3615  0.4338  0.5061  0.5784  0.6507   \n",
              "47  0.0804  0.1608  0.2412  0.3216  0.4020  0.4824  0.5628  0.6432  0.7236   \n",
              "48  0.0390  0.0780  0.1170  0.1560  0.1950  0.2340  0.2730  0.3120  0.3510   \n",
              "49  0.0470  0.0940  0.1410  0.1880  0.2350  0.2820  0.3290  0.3760  0.4230   \n",
              "\n",
              "       9   ...      45      46      47      48      49      50      51  \\\n",
              "0   0.662  ...  0.0008  0.0000  0.0000  0.0000  0.0008  0.0004  0.0776   \n",
              "1   0.662  ...  0.0004  0.0000  0.0004  0.0000  0.0008  0.0008  0.0792   \n",
              "2   2.692  ...  0.0000  0.0021  0.0010  0.0000  0.0052  0.0010  0.2884   \n",
              "3   0.380  ...  0.0000  0.0000  0.0000  0.0120  0.0176  0.0060  0.0056   \n",
              "4   0.383  ...  0.0000  0.0008  0.0000  0.0116  0.0180  0.0084  0.0032   \n",
              "5   0.373  ...  0.0004  0.0000  0.0000  0.0000  0.0168  0.0136  0.0092   \n",
              "6   0.380  ...  0.0000  0.0004  0.0004  0.0096  0.0168  0.0104  0.0048   \n",
              "7   0.376  ...  0.0008  0.0000  0.0000  0.0040  0.0204  0.0112  0.0064   \n",
              "8   0.379  ...  0.0008  0.0000  0.0000  0.0076  0.0168  0.0108  0.0048   \n",
              "9   0.376  ...  0.0004  0.0004  0.0000  0.0040  0.0196  0.0092  0.0076   \n",
              "10  0.584  ...  0.0260  0.0096  0.0108  0.0060  0.0036  0.0024  0.0144   \n",
              "11  0.584  ...  0.0264  0.0076  0.0132  0.0032  0.0036  0.0056  0.0140   \n",
              "12  0.247  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004  0.0012   \n",
              "13  0.146  ...  0.0244  0.0100  0.0196  0.0108  0.0040  0.0104  0.0024   \n",
              "14  0.172  ...  0.0080  0.0028  0.0080  0.0016  0.0000  0.0000  0.0004   \n",
              "15  0.363  ...  0.0012  0.0004  0.0000  0.0016  0.0000  0.0004  0.0368   \n",
              "16  0.363  ...  0.0000  0.0000  0.0000  0.0000  0.0004  0.0000  0.0424   \n",
              "17  0.363  ...  0.0008  0.0000  0.0000  0.0004  0.0000  0.0004  0.0424   \n",
              "18  0.363  ...  0.0000  0.0008  0.0000  0.0004  0.0004  0.0000  0.0360   \n",
              "19  0.088  ...  0.0004  0.0000  0.0004  0.0000  0.0000  0.0000  0.0004   \n",
              "20  0.067  ...  0.0084  0.0064  0.0008  0.0032  0.0016  0.0004  0.0004   \n",
              "21  0.071  ...  0.0044  0.0048  0.0000  0.0008  0.0004  0.0012  0.0004   \n",
              "22  0.074  ...  0.0024  0.0000  0.0008  0.0004  0.0004  0.0004  0.0004   \n",
              "23  0.067  ...  0.0136  0.0048  0.0004  0.0028  0.0016  0.0008  0.0008   \n",
              "24  0.067  ...  0.0116  0.0064  0.0004  0.0024  0.0012  0.0000  0.0008   \n",
              "25  0.071  ...  0.0060  0.0032  0.0004  0.0000  0.0004  0.0000  0.0004   \n",
              "26  0.078  ...  0.0012  0.0020  0.0020  0.0012  0.0012  0.0000  0.0004   \n",
              "27  0.067  ...  0.0088  0.0076  0.0016  0.0016  0.0024  0.0016  0.0012   \n",
              "28  0.085  ...  0.0012  0.0004  0.0004  0.0004  0.0004  0.0000  0.0004   \n",
              "29  0.085  ...  0.0020  0.0000  0.0000  0.0000  0.0004  0.0004  0.0004   \n",
              "30  0.074  ...  0.0076  0.0004  0.0012  0.0012  0.0004  0.0004  0.0004   \n",
              "31  0.080  ...  0.0032  0.0012  0.0012  0.0004  0.0000  0.0000  0.0012   \n",
              "32  0.073  ...  0.0068  0.0040  0.0016  0.0024  0.0008  0.0000  0.0008   \n",
              "33  0.081  ...  0.0036  0.0028  0.0016  0.0000  0.0000  0.0004  0.0008   \n",
              "34  0.109  ...  0.0016  0.0012  0.0004  0.0000  0.0000  0.0000  0.0004   \n",
              "35  0.122  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004  0.0004   \n",
              "36  0.101  ...  0.0028  0.0000  0.0008  0.0000  0.0000  0.0000  0.0004   \n",
              "37  0.111  ...  0.0020  0.0004  0.0008  0.0004  0.0000  0.0000  0.0008   \n",
              "38  0.104  ...  0.0008  0.0000  0.0000  0.0004  0.0004  0.0000  0.0004   \n",
              "39  0.118  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "40  0.078  ...  0.0000  0.0008  0.0000  0.0000  0.0008  0.0004  0.0004   \n",
              "41  0.074  ...  0.0016  0.0000  0.0008  0.0012  0.0000  0.0000  0.0004   \n",
              "42  0.060  ...  0.0044  0.0272  0.0104  0.0000  0.0020  0.0016  0.0004   \n",
              "43  0.072  ...  0.0044  0.0004  0.0004  0.0000  0.0004  0.0000  0.0004   \n",
              "44  0.067  ...  0.0100  0.0032  0.0000  0.0024  0.0004  0.0004  0.0004   \n",
              "45  1.557  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "46  0.723  ...  0.0000  0.0000  0.0004  0.0004  0.0000  0.0284  0.0320   \n",
              "47  0.804  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004  0.0020   \n",
              "48  0.390  ...  0.0000  0.0000  0.0000  0.0004  0.0000  0.0000  0.0304   \n",
              "49  0.470  ...  0.0000  0.0000  0.0000  0.0028  0.0284  0.0088  0.0044   \n",
              "\n",
              "        52      53  54  \n",
              "0   0.2282  0.9463   0  \n",
              "1   0.2293  0.9524   0  \n",
              "2   1.5919  0.9828   0  \n",
              "3   0.1430  0.9826   0  \n",
              "4   0.1428  0.9810   0  \n",
              "5   0.1428  0.9810   0  \n",
              "6   0.1438  0.9873   0  \n",
              "7   0.1432  0.9841   0  \n",
              "8   0.1421  0.9778   0  \n",
              "9   0.1430  0.9826   0  \n",
              "10  0.1738  0.9928   0  \n",
              "11  0.1733  0.9944   0  \n",
              "12  0.0795  0.9662   0  \n",
              "13  0.0785  0.9662   0  \n",
              "14  0.0786  0.9662   0  \n",
              "15  0.1093  0.9670   0  \n",
              "16  0.1085  0.9778   0  \n",
              "17  0.1082  0.9794   0  \n",
              "18  0.1065  0.9685   0  \n",
              "19  0.0454  0.7301   1  \n",
              "20  0.0466  0.7100   1  \n",
              "21  0.0471  0.6938   1  \n",
              "22  0.0465  0.7030   1  \n",
              "23  0.0474  0.6984   1  \n",
              "24  0.0467  0.7100   1  \n",
              "25  0.0468  0.7100   1  \n",
              "26  0.0454  0.7801   1  \n",
              "27  0.0456  0.7687   1  \n",
              "28  0.0444  0.7788   1  \n",
              "29  0.0451  0.7813   1  \n",
              "30  0.0453  0.7813   1  \n",
              "31  0.0459  0.7738   1  \n",
              "32  0.0453  0.7750   1  \n",
              "33  0.0464  0.7687   1  \n",
              "34  0.0467  0.8622   1  \n",
              "35  0.0458  0.8636   1  \n",
              "36  0.0462  0.8609   1  \n",
              "37  0.0460  0.8790   1  \n",
              "38  0.0444  0.8790   1  \n",
              "39  0.0486  0.6667   1  \n",
              "40  0.0497  0.6351   1  \n",
              "41  0.0484  0.6513   1  \n",
              "42  0.0495  0.6372   1  \n",
              "43  0.0491  0.6437   1  \n",
              "44  0.0485  0.6469   1  \n",
              "45  0.2532  0.8918   0  \n",
              "46  0.2474  0.8828   0  \n",
              "47  0.1507  0.9673   0  \n",
              "48  0.1440  0.9789   0  \n",
              "49  0.1806  0.9697   0  \n",
              "\n",
              "[50 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8bc416e-83c1-4517-a8e4-6cc00d448e04\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0662</td>\n",
              "      <td>0.1324</td>\n",
              "      <td>0.1986</td>\n",
              "      <td>0.2648</td>\n",
              "      <td>0.3310</td>\n",
              "      <td>0.3972</td>\n",
              "      <td>0.4634</td>\n",
              "      <td>0.5296</td>\n",
              "      <td>0.5958</td>\n",
              "      <td>0.662</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>0.2282</td>\n",
              "      <td>0.9463</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0662</td>\n",
              "      <td>0.1324</td>\n",
              "      <td>0.1986</td>\n",
              "      <td>0.2648</td>\n",
              "      <td>0.3310</td>\n",
              "      <td>0.3972</td>\n",
              "      <td>0.4634</td>\n",
              "      <td>0.5296</td>\n",
              "      <td>0.5958</td>\n",
              "      <td>0.662</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0792</td>\n",
              "      <td>0.2293</td>\n",
              "      <td>0.9524</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2692</td>\n",
              "      <td>0.5384</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>1.0768</td>\n",
              "      <td>1.3460</td>\n",
              "      <td>1.6152</td>\n",
              "      <td>1.8844</td>\n",
              "      <td>2.1536</td>\n",
              "      <td>2.4228</td>\n",
              "      <td>2.692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.2884</td>\n",
              "      <td>1.5919</td>\n",
              "      <td>0.9828</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0380</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.1520</td>\n",
              "      <td>0.1900</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2660</td>\n",
              "      <td>0.3040</td>\n",
              "      <td>0.3420</td>\n",
              "      <td>0.380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.9826</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0766</td>\n",
              "      <td>0.1149</td>\n",
              "      <td>0.1532</td>\n",
              "      <td>0.1915</td>\n",
              "      <td>0.2298</td>\n",
              "      <td>0.2681</td>\n",
              "      <td>0.3064</td>\n",
              "      <td>0.3447</td>\n",
              "      <td>0.383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.1428</td>\n",
              "      <td>0.9810</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0373</td>\n",
              "      <td>0.0746</td>\n",
              "      <td>0.1119</td>\n",
              "      <td>0.1492</td>\n",
              "      <td>0.1865</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.2611</td>\n",
              "      <td>0.2984</td>\n",
              "      <td>0.3357</td>\n",
              "      <td>0.373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.1428</td>\n",
              "      <td>0.9810</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0380</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.1520</td>\n",
              "      <td>0.1900</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2660</td>\n",
              "      <td>0.3040</td>\n",
              "      <td>0.3420</td>\n",
              "      <td>0.380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.1438</td>\n",
              "      <td>0.9873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0376</td>\n",
              "      <td>0.0752</td>\n",
              "      <td>0.1128</td>\n",
              "      <td>0.1504</td>\n",
              "      <td>0.1880</td>\n",
              "      <td>0.2256</td>\n",
              "      <td>0.2632</td>\n",
              "      <td>0.3008</td>\n",
              "      <td>0.3384</td>\n",
              "      <td>0.376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0204</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.1432</td>\n",
              "      <td>0.9841</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0379</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.1516</td>\n",
              "      <td>0.1895</td>\n",
              "      <td>0.2274</td>\n",
              "      <td>0.2653</td>\n",
              "      <td>0.3032</td>\n",
              "      <td>0.3411</td>\n",
              "      <td>0.379</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.1421</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0376</td>\n",
              "      <td>0.0752</td>\n",
              "      <td>0.1128</td>\n",
              "      <td>0.1504</td>\n",
              "      <td>0.1880</td>\n",
              "      <td>0.2256</td>\n",
              "      <td>0.2632</td>\n",
              "      <td>0.3008</td>\n",
              "      <td>0.3384</td>\n",
              "      <td>0.376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0196</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.9826</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.1168</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.2336</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>0.3504</td>\n",
              "      <td>0.4088</td>\n",
              "      <td>0.4672</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0144</td>\n",
              "      <td>0.1738</td>\n",
              "      <td>0.9928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.1168</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.2336</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>0.3504</td>\n",
              "      <td>0.4088</td>\n",
              "      <td>0.4672</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0132</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.1733</td>\n",
              "      <td>0.9944</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0247</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.0988</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>0.1482</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.1976</td>\n",
              "      <td>0.2223</td>\n",
              "      <td>0.247</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0795</td>\n",
              "      <td>0.9662</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0438</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0730</td>\n",
              "      <td>0.0876</td>\n",
              "      <td>0.1022</td>\n",
              "      <td>0.1168</td>\n",
              "      <td>0.1314</td>\n",
              "      <td>0.146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0196</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0785</td>\n",
              "      <td>0.9662</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0344</td>\n",
              "      <td>0.0516</td>\n",
              "      <td>0.0688</td>\n",
              "      <td>0.0860</td>\n",
              "      <td>0.1032</td>\n",
              "      <td>0.1204</td>\n",
              "      <td>0.1376</td>\n",
              "      <td>0.1548</td>\n",
              "      <td>0.172</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0786</td>\n",
              "      <td>0.9662</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.1452</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>0.2178</td>\n",
              "      <td>0.2541</td>\n",
              "      <td>0.2904</td>\n",
              "      <td>0.3267</td>\n",
              "      <td>0.363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>0.9670</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.1452</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>0.2178</td>\n",
              "      <td>0.2541</td>\n",
              "      <td>0.2904</td>\n",
              "      <td>0.3267</td>\n",
              "      <td>0.363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0424</td>\n",
              "      <td>0.1085</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.1452</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>0.2178</td>\n",
              "      <td>0.2541</td>\n",
              "      <td>0.2904</td>\n",
              "      <td>0.3267</td>\n",
              "      <td>0.363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0424</td>\n",
              "      <td>0.1082</td>\n",
              "      <td>0.9794</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.1452</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>0.2178</td>\n",
              "      <td>0.2541</td>\n",
              "      <td>0.2904</td>\n",
              "      <td>0.3267</td>\n",
              "      <td>0.363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.1065</td>\n",
              "      <td>0.9685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0440</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0616</td>\n",
              "      <td>0.0704</td>\n",
              "      <td>0.0792</td>\n",
              "      <td>0.088</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.7301</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>0.0335</td>\n",
              "      <td>0.0402</td>\n",
              "      <td>0.0469</td>\n",
              "      <td>0.0536</td>\n",
              "      <td>0.0603</td>\n",
              "      <td>0.067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0466</td>\n",
              "      <td>0.7100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0213</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0355</td>\n",
              "      <td>0.0426</td>\n",
              "      <td>0.0497</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0639</td>\n",
              "      <td>0.071</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0471</td>\n",
              "      <td>0.6938</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0074</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.0370</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.0518</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.074</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.7030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>0.0335</td>\n",
              "      <td>0.0402</td>\n",
              "      <td>0.0469</td>\n",
              "      <td>0.0536</td>\n",
              "      <td>0.0603</td>\n",
              "      <td>0.067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>0.6984</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>0.0335</td>\n",
              "      <td>0.0402</td>\n",
              "      <td>0.0469</td>\n",
              "      <td>0.0536</td>\n",
              "      <td>0.0603</td>\n",
              "      <td>0.067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0467</td>\n",
              "      <td>0.7100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0213</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0355</td>\n",
              "      <td>0.0426</td>\n",
              "      <td>0.0497</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0639</td>\n",
              "      <td>0.071</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0468</td>\n",
              "      <td>0.7100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0078</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0234</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0390</td>\n",
              "      <td>0.0468</td>\n",
              "      <td>0.0546</td>\n",
              "      <td>0.0624</td>\n",
              "      <td>0.0702</td>\n",
              "      <td>0.078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.7801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>0.0335</td>\n",
              "      <td>0.0402</td>\n",
              "      <td>0.0469</td>\n",
              "      <td>0.0536</td>\n",
              "      <td>0.0603</td>\n",
              "      <td>0.067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0456</td>\n",
              "      <td>0.7687</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0170</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>0.0340</td>\n",
              "      <td>0.0425</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.0595</td>\n",
              "      <td>0.0680</td>\n",
              "      <td>0.0765</td>\n",
              "      <td>0.085</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.7788</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0170</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>0.0340</td>\n",
              "      <td>0.0425</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.0595</td>\n",
              "      <td>0.0680</td>\n",
              "      <td>0.0765</td>\n",
              "      <td>0.085</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.0074</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.0370</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.0518</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.074</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0320</td>\n",
              "      <td>0.0400</td>\n",
              "      <td>0.0480</td>\n",
              "      <td>0.0560</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>0.0720</td>\n",
              "      <td>0.080</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0459</td>\n",
              "      <td>0.7738</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0219</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0365</td>\n",
              "      <td>0.0438</td>\n",
              "      <td>0.0511</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0657</td>\n",
              "      <td>0.073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.7750</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>0.0243</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0405</td>\n",
              "      <td>0.0486</td>\n",
              "      <td>0.0567</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>0.0729</td>\n",
              "      <td>0.081</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0464</td>\n",
              "      <td>0.7687</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.0109</td>\n",
              "      <td>0.0218</td>\n",
              "      <td>0.0327</td>\n",
              "      <td>0.0436</td>\n",
              "      <td>0.0545</td>\n",
              "      <td>0.0654</td>\n",
              "      <td>0.0763</td>\n",
              "      <td>0.0872</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.109</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0467</td>\n",
              "      <td>0.8622</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.0122</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0366</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>0.0610</td>\n",
              "      <td>0.0732</td>\n",
              "      <td>0.0854</td>\n",
              "      <td>0.0976</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.122</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>0.8636</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0303</td>\n",
              "      <td>0.0404</td>\n",
              "      <td>0.0505</td>\n",
              "      <td>0.0606</td>\n",
              "      <td>0.0707</td>\n",
              "      <td>0.0808</td>\n",
              "      <td>0.0909</td>\n",
              "      <td>0.101</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.8609</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.0333</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.0555</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0777</td>\n",
              "      <td>0.0888</td>\n",
              "      <td>0.0999</td>\n",
              "      <td>0.111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0460</td>\n",
              "      <td>0.8790</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0208</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0416</td>\n",
              "      <td>0.0520</td>\n",
              "      <td>0.0624</td>\n",
              "      <td>0.0728</td>\n",
              "      <td>0.0832</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>0.104</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.8790</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0236</td>\n",
              "      <td>0.0354</td>\n",
              "      <td>0.0472</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0708</td>\n",
              "      <td>0.0826</td>\n",
              "      <td>0.0944</td>\n",
              "      <td>0.1062</td>\n",
              "      <td>0.118</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0486</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.0078</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0234</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0390</td>\n",
              "      <td>0.0468</td>\n",
              "      <td>0.0546</td>\n",
              "      <td>0.0624</td>\n",
              "      <td>0.0702</td>\n",
              "      <td>0.078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0497</td>\n",
              "      <td>0.6351</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.0074</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.0370</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.0518</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.074</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.6513</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0300</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0420</td>\n",
              "      <td>0.0480</td>\n",
              "      <td>0.0540</td>\n",
              "      <td>0.060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.6372</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0144</td>\n",
              "      <td>0.0216</td>\n",
              "      <td>0.0288</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0432</td>\n",
              "      <td>0.0504</td>\n",
              "      <td>0.0576</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>0.072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0491</td>\n",
              "      <td>0.6437</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>0.0335</td>\n",
              "      <td>0.0402</td>\n",
              "      <td>0.0469</td>\n",
              "      <td>0.0536</td>\n",
              "      <td>0.0603</td>\n",
              "      <td>0.067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.6469</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.1557</td>\n",
              "      <td>0.3114</td>\n",
              "      <td>0.4671</td>\n",
              "      <td>0.6228</td>\n",
              "      <td>0.7785</td>\n",
              "      <td>0.9342</td>\n",
              "      <td>1.0899</td>\n",
              "      <td>1.2456</td>\n",
              "      <td>1.4013</td>\n",
              "      <td>1.557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.2532</td>\n",
              "      <td>0.8918</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.0723</td>\n",
              "      <td>0.1446</td>\n",
              "      <td>0.2169</td>\n",
              "      <td>0.2892</td>\n",
              "      <td>0.3615</td>\n",
              "      <td>0.4338</td>\n",
              "      <td>0.5061</td>\n",
              "      <td>0.5784</td>\n",
              "      <td>0.6507</td>\n",
              "      <td>0.723</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0320</td>\n",
              "      <td>0.2474</td>\n",
              "      <td>0.8828</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.0804</td>\n",
              "      <td>0.1608</td>\n",
              "      <td>0.2412</td>\n",
              "      <td>0.3216</td>\n",
              "      <td>0.4020</td>\n",
              "      <td>0.4824</td>\n",
              "      <td>0.5628</td>\n",
              "      <td>0.6432</td>\n",
              "      <td>0.7236</td>\n",
              "      <td>0.804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.1507</td>\n",
              "      <td>0.9673</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.0390</td>\n",
              "      <td>0.0780</td>\n",
              "      <td>0.1170</td>\n",
              "      <td>0.1560</td>\n",
              "      <td>0.1950</td>\n",
              "      <td>0.2340</td>\n",
              "      <td>0.2730</td>\n",
              "      <td>0.3120</td>\n",
              "      <td>0.3510</td>\n",
              "      <td>0.390</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.9789</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.0470</td>\n",
              "      <td>0.0940</td>\n",
              "      <td>0.1410</td>\n",
              "      <td>0.1880</td>\n",
              "      <td>0.2350</td>\n",
              "      <td>0.2820</td>\n",
              "      <td>0.3290</td>\n",
              "      <td>0.3760</td>\n",
              "      <td>0.4230</td>\n",
              "      <td>0.470</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.1806</td>\n",
              "      <td>0.9697</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50 rows Ã— 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8bc416e-83c1-4517-a8e4-6cc00d448e04')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8bc416e-83c1-4517-a8e4-6cc00d448e04 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8bc416e-83c1-4517-a8e4-6cc00d448e04');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqT_lL8Xp3MW"
      },
      "outputs": [],
      "source": [
        "y_actual = df.iloc[:,54].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egw8m5tzp3MZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaXXPlBxp3Mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b032a3d-01b1-4181-9777-9789ea31f7a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "y_actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPxIYtCep3Ms"
      },
      "outputs": [],
      "source": [
        "X_test = df.iloc[:,0:26].to_numpy()\n",
        "Y_test = df.iloc[:,26:52].to_numpy()\n",
        "IPI = df.iloc[:,52].to_numpy()\n",
        "col = df.iloc[:,53].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqkn2UjOp3Ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97db47ac-d45b-459c-c594-b79bff6e0602"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YcEhtPdp3Ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2e8a25-beb7-49af-8130-405d169327cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGCfap2tp3Mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3d0a89-fd07-4669-fba5-21a365a05173"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A258u5Pjp3Mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27415d5d-2d42-4234-f0e2-6306826a1238"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "IPI.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrGvMBUSp3Mt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "6bdd673c-9069-4188-d105-1e784650d5d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c250867682c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIPI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict([X_test, Y_test, np.transpose(IPI), np.transpose(col)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX3OpF1yp3Mu"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KU2-FqWp3Mu"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlfxInUep3Mu"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_actual, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw_8k_qip3Mu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6MNJ4REp3Mu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}