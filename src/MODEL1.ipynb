{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7sB3khuietfP"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python.exe -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0I8ylbae7pa",
        "outputId": "c1568ef7-4227-40e8-f0fa-36a4cbfc5a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting graphviz\n",
            "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
            "     ---------------------------------------- 0.0/47.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 47.0/47.0 kB 1.2 MB/s eta 0:00:00\n",
            "Installing collected packages: graphviz\n",
            "Successfully installed graphviz-0.20.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7rGiH6WTfB5-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sahil\\Sahil's Data\\CODES\\GitHub\\ML-Enabled-WiFi-Saturation-Sensing\\src\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mK4Q01xpiPO_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"dataset/dataset_new.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "GAo836l0ixWh",
        "outputId": "16b5ff78-353d-41c0-8ca4-9b79684f9abe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0325</td>\n",
              "      <td>0.0390</td>\n",
              "      <td>0.0455</td>\n",
              "      <td>0.0520</td>\n",
              "      <td>0.0585</td>\n",
              "      <td>0.0650</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9960</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0263</td>\n",
              "      <td>0.0526</td>\n",
              "      <td>0.0789</td>\n",
              "      <td>0.1052</td>\n",
              "      <td>0.1316</td>\n",
              "      <td>0.1579</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.2105</td>\n",
              "      <td>0.2368</td>\n",
              "      <td>0.2631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9960</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0704</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>0.1409</td>\n",
              "      <td>0.1585</td>\n",
              "      <td>0.1761</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0299</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.0444</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0888</td>\n",
              "      <td>0.1110</td>\n",
              "      <td>0.1332</td>\n",
              "      <td>0.1554</td>\n",
              "      <td>0.1776</td>\n",
              "      <td>0.1998</td>\n",
              "      <td>0.2220</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0307</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9900</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0189</td>\n",
              "      <td>0.0379</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.0947</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.1326</td>\n",
              "      <td>0.1516</td>\n",
              "      <td>0.1705</td>\n",
              "      <td>0.1895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.0312</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.9862</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>0.0639</td>\n",
              "      <td>0.1278</td>\n",
              "      <td>0.1918</td>\n",
              "      <td>0.2557</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.4474</td>\n",
              "      <td>0.5114</td>\n",
              "      <td>0.5753</td>\n",
              "      <td>0.6392</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.1793</td>\n",
              "      <td>0.9856</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>0.0830</td>\n",
              "      <td>0.1659</td>\n",
              "      <td>0.2489</td>\n",
              "      <td>0.3318</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4977</td>\n",
              "      <td>0.5807</td>\n",
              "      <td>0.6637</td>\n",
              "      <td>0.7466</td>\n",
              "      <td>0.8296</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.1793</td>\n",
              "      <td>0.9849</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>0.0854</td>\n",
              "      <td>0.1709</td>\n",
              "      <td>0.2563</td>\n",
              "      <td>0.3417</td>\n",
              "      <td>0.4272</td>\n",
              "      <td>0.5126</td>\n",
              "      <td>0.5980</td>\n",
              "      <td>0.6835</td>\n",
              "      <td>0.7689</td>\n",
              "      <td>0.8543</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.1793</td>\n",
              "      <td>0.9825</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>0.0648</td>\n",
              "      <td>0.1296</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.2593</td>\n",
              "      <td>0.3241</td>\n",
              "      <td>0.3889</td>\n",
              "      <td>0.4537</td>\n",
              "      <td>0.5186</td>\n",
              "      <td>0.5834</td>\n",
              "      <td>0.6482</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.1793</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>0.3579</td>\n",
              "      <td>0.7158</td>\n",
              "      <td>1.0737</td>\n",
              "      <td>1.4316</td>\n",
              "      <td>1.7895</td>\n",
              "      <td>2.1473</td>\n",
              "      <td>2.5052</td>\n",
              "      <td>2.8631</td>\n",
              "      <td>3.2210</td>\n",
              "      <td>3.5789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.3056</td>\n",
              "      <td>0.9166</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0       1       2       3       4       5       6       7       8   \\\n",
              "0      0.0065  0.0130  0.0195  0.0260  0.0325  0.0390  0.0455  0.0520  0.0585   \n",
              "1      0.0263  0.0526  0.0789  0.1052  0.1316  0.1579  0.1842  0.2105  0.2368   \n",
              "2      0.0176  0.0352  0.0528  0.0704  0.0881  0.1057  0.1233  0.1409  0.1585   \n",
              "3      0.0222  0.0444  0.0666  0.0888  0.1110  0.1332  0.1554  0.1776  0.1998   \n",
              "4      0.0189  0.0379  0.0568  0.0758  0.0947  0.1137  0.1326  0.1516  0.1705   \n",
              "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "19995  0.0639  0.1278  0.1918  0.2557  0.3196  0.3835  0.4474  0.5114  0.5753   \n",
              "19996  0.0830  0.1659  0.2489  0.3318  0.4148  0.4977  0.5807  0.6637  0.7466   \n",
              "19997  0.0854  0.1709  0.2563  0.3417  0.4272  0.5126  0.5980  0.6835  0.7689   \n",
              "19998  0.0648  0.1296  0.1945  0.2593  0.3241  0.3889  0.4537  0.5186  0.5834   \n",
              "19999  0.3579  0.7158  1.0737  1.4316  1.7895  2.1473  2.5052  2.8631  3.2210   \n",
              "\n",
              "           9   ...      45      46      47      48      49      50      51  \\\n",
              "0      0.0650  ...  0.0316  0.0368  0.0312  0.0000  0.0316  0.0364  0.0304   \n",
              "1      0.2631  ...  0.0316  0.0368  0.0312  0.0000  0.0316  0.0364  0.0304   \n",
              "2      0.1761  ...  0.0316  0.0368  0.0312  0.0312  0.0316  0.0299  0.0304   \n",
              "3      0.2220  ...  0.0316  0.0307  0.0312  0.0000  0.0360  0.0364  0.0304   \n",
              "4      0.1895  ...  0.0316  0.0368  0.0312  0.0000  0.0316  0.0364  0.0304   \n",
              "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "19995  0.6392  ...  0.0000  0.0000  0.0000  0.0028  0.0284  0.0076  0.0048   \n",
              "19996  0.8296  ...  0.0000  0.0000  0.0000  0.0028  0.0284  0.0076  0.0048   \n",
              "19997  0.8543  ...  0.0000  0.0000  0.0000  0.0028  0.0284  0.0076  0.0048   \n",
              "19998  0.6482  ...  0.0000  0.0000  0.0000  0.0277  0.0284  0.0076  0.0048   \n",
              "19999  3.5789  ...  0.0000  0.0017  0.0027  0.0033  0.0000  0.0001  0.0034   \n",
              "\n",
              "           52      53  54  \n",
              "0      0.0604  0.9960   1  \n",
              "1      0.0604  0.9960   1  \n",
              "2      0.0604  0.9957   1  \n",
              "3      0.0604  0.9900   1  \n",
              "4      0.0604  0.9862   1  \n",
              "...       ...     ...  ..  \n",
              "19995  0.1793  0.9856   0  \n",
              "19996  0.1793  0.9849   0  \n",
              "19997  0.1793  0.9825   0  \n",
              "19998  0.1793  0.9782   0  \n",
              "19999  0.3056  0.9166   0  \n",
              "\n",
              "[20000 rows x 55 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDnJtBJ9i40B",
        "outputId": "5cd34475-19d0-4633-b238-4c7e7875649d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000, 55)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ANGvCKlmi6jz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten, Conv2D, BatchNormalization, MaxPooling2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XsoT2wrojKUP"
      },
      "outputs": [],
      "source": [
        "def normalize_data(data):\n",
        "    data = np.transpose(data)\n",
        "    scalar = MinMaxScaler((0,1))\n",
        "    scalar.fit(data)\n",
        "    data = scalar.transform(data)\n",
        "    output = np.transpose(data)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Iy4tRtkBjWbC"
      },
      "outputs": [],
      "source": [
        "classes = ['Unsaturated', 'Saturated']\n",
        "num_classes = len(classes)\n",
        "in_dim = [2, 13]\n",
        "X = df.iloc[:,0:26].to_numpy()\n",
        "Y = df.iloc[:,26:52].to_numpy()\n",
        "Label = df.iloc[:,54].to_numpy()\n",
        "Label_cat = tf.keras.utils.to_categorical(Label,num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MmmLhWSJjf7z"
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed=int(time.time()))\n",
        "n_examples = X.shape[0]\n",
        "n_train = n_examples * 0.7\n",
        "train_idx = np.random.choice(range(0,n_examples), size=int(n_train), replace=False)\n",
        "test_idx = list(set(range(0,n_examples))-set(train_idx))\n",
        "X_train = X[train_idx]\n",
        "X_test = X[test_idx]\n",
        "Y_train = Y[train_idx]\n",
        "Y_test = Y[test_idx]\n",
        "Label_train = Label_cat[train_idx]\n",
        "Label_test = Label_cat[test_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z9us9yyNjs_3"
      },
      "outputs": [],
      "source": [
        "def get_model(X_shape, Y_shape, classes, verbose):\n",
        "\n",
        "    dr = 0.40\n",
        "    model_X = tf.keras.models.Sequential()\n",
        "    model_X.add(layers.Reshape((X_shape, 1), input_shape=(X_shape, ) ))\n",
        "    model_X.add(layers.Conv1D(32, 3, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.BatchNormalization())\n",
        "\n",
        "    model_X.add(layers.Conv1D(16, 2, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.Flatten())\n",
        "    model_X.add(layers.Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.Dropout(dr))\n",
        "    model_X.add(layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_X.add(layers.Dropout(dr))\n",
        "\n",
        "    model_Y = tf.keras.models.Sequential()\n",
        "    model_Y.add(layers.Reshape((Y_shape, 1), input_shape=(Y_shape, ) ))\n",
        "    model_Y.add(layers.Conv1D(32, 3, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01) ))\n",
        "    model_Y.add(layers.BatchNormalization())\n",
        "\n",
        "    model_Y.add(layers.Conv1D(16, 2, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_Y.add(layers.Flatten())\n",
        "    model_Y.add(layers.Dropout(dr))\n",
        "    model_Y.add(layers.Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01) ))\n",
        "    model_Y.add(layers.Dropout(dr))\n",
        "    model_Y.add(layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model_Y.add(layers.Dropout(dr))\n",
        "\n",
        "\n",
        "    merged = tf.keras.layers.Concatenate(axis=1)([model_X.output, model_Y.output])\n",
        "    merged = layers.Dense(256,activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(merged)\n",
        "    merged = layers.Dropout(dr)(merged)\n",
        "    merged = layers.Dense(128,activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(merged)\n",
        "    merged = layers.Dropout(dr)(merged)\n",
        "    merged = layers.Dense(len(classes), activation=tf.nn.softmax, name='class_output')(merged)\n",
        "\n",
        "    model = tf.keras.models.Model([model_X.input,model_Y.input], merged)\n",
        "\n",
        "    if(verbose):\n",
        "        from tensorflow.keras.utils import plot_model\n",
        "        from IPython.display import Image\n",
        "        plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "    print(\"--Model created\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OUaGfIJj1nu",
        "outputId": "397c7cc6-754a-4543-ca99-d4690294893c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
            "--Model created\n"
          ]
        }
      ],
      "source": [
        "model = get_model(X.shape[1], Y.shape[1], classes, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLhO1eVvj5sR",
        "outputId": "87f15b6a-6429-4081-fb96-61d353d36ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " reshape_1_input (InputLayer)   [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " reshape_input (InputLayer)     [(None, 26)]         0           []                               \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 26, 1)        0           ['reshape_1_input[0][0]']        \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 26, 1)        0           ['reshape_input[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 24, 32)       128         ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 24, 32)       128         ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 24, 32)      128         ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 24, 32)      128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 23, 16)       1040        ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 23, 16)       1040        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 368)          0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 368)          0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 368)          0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          94464       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          94464       ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          32896       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128)          32896       ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 256)          0           ['dropout_1[0][0]',              \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 256)          65792       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 256)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 128)          32896       ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 128)          0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " class_output (Dense)           (None, 2)            258         ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 356,258\n",
            "Trainable params: 356,130\n",
            "Non-trainable params: 128\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4dCVLVTj9m6",
        "outputId": "59adf217-aa0c-4623-a8d0-d502258afeab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss= 'categorical_crossentropy', metrics=['accuracy']  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ExzmrChkCRh",
        "outputId": "ba4266a0-c8e6-4dce-fbf0-a0af953395c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5000\n",
            "\n",
            "Epoch 1: loss improved from inf to 1.46817, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 9s - loss: 1.4682 - accuracy: 0.9334 - val_loss: 0.2715 - val_accuracy: 0.9598 - lr: 0.0010 - 9s/epoch - 10ms/step\n",
            "Epoch 2/5000\n",
            "\n",
            "Epoch 2: loss improved from 1.46817 to 0.26801, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2680 - accuracy: 0.9522 - val_loss: 0.2337 - val_accuracy: 0.9547 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/5000\n",
            "\n",
            "Epoch 3: loss improved from 0.26801 to 0.25061, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2506 - accuracy: 0.9520 - val_loss: 0.2446 - val_accuracy: 0.9538 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/5000\n",
            "\n",
            "Epoch 4: loss improved from 0.25061 to 0.23669, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2367 - accuracy: 0.9545 - val_loss: 0.2135 - val_accuracy: 0.9693 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/5000\n",
            "\n",
            "Epoch 5: loss improved from 0.23669 to 0.23056, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2306 - accuracy: 0.9554 - val_loss: 0.2173 - val_accuracy: 0.9548 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
            "Epoch 6/5000\n",
            "\n",
            "Epoch 6: loss improved from 0.23056 to 0.22575, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2258 - accuracy: 0.9554 - val_loss: 0.1851 - val_accuracy: 0.9672 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
            "Epoch 7/5000\n",
            "\n",
            "Epoch 7: loss improved from 0.22575 to 0.21290, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2129 - accuracy: 0.9598 - val_loss: 0.1873 - val_accuracy: 0.9657 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/5000\n",
            "\n",
            "Epoch 8: loss did not improve from 0.21290\n",
            "875/875 - 6s - loss: 0.2153 - accuracy: 0.9591 - val_loss: 0.1744 - val_accuracy: 0.9753 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/5000\n",
            "\n",
            "Epoch 9: loss improved from 0.21290 to 0.20937, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.2094 - accuracy: 0.9616 - val_loss: 0.1936 - val_accuracy: 0.9647 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 10/5000\n",
            "\n",
            "Epoch 10: loss improved from 0.20937 to 0.20490, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.2049 - accuracy: 0.9619 - val_loss: 0.1677 - val_accuracy: 0.9732 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 11/5000\n",
            "\n",
            "Epoch 11: loss improved from 0.20490 to 0.20351, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2035 - accuracy: 0.9613 - val_loss: 0.1738 - val_accuracy: 0.9718 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/5000\n",
            "\n",
            "Epoch 12: loss improved from 0.20351 to 0.20280, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.2028 - accuracy: 0.9626 - val_loss: 0.1713 - val_accuracy: 0.9733 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
            "Epoch 13/5000\n",
            "\n",
            "Epoch 13: loss improved from 0.20280 to 0.19522, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.1952 - accuracy: 0.9628 - val_loss: 0.1720 - val_accuracy: 0.9762 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 14/5000\n",
            "\n",
            "Epoch 14: loss did not improve from 0.19522\n",
            "875/875 - 5s - loss: 0.1976 - accuracy: 0.9626 - val_loss: 0.1719 - val_accuracy: 0.9718 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 15/5000\n",
            "\n",
            "Epoch 15: loss improved from 0.19522 to 0.19196, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1920 - accuracy: 0.9641 - val_loss: 0.1994 - val_accuracy: 0.9545 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 16/5000\n",
            "\n",
            "Epoch 16: loss did not improve from 0.19196\n",
            "875/875 - 5s - loss: 0.1948 - accuracy: 0.9620 - val_loss: 0.1637 - val_accuracy: 0.9745 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 17/5000\n",
            "\n",
            "Epoch 17: loss did not improve from 0.19196\n",
            "875/875 - 5s - loss: 0.1936 - accuracy: 0.9639 - val_loss: 0.1657 - val_accuracy: 0.9747 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 18/5000\n",
            "\n",
            "Epoch 18: loss improved from 0.19196 to 0.19146, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.1915 - accuracy: 0.9624 - val_loss: 0.1717 - val_accuracy: 0.9730 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 19/5000\n",
            "\n",
            "Epoch 19: loss did not improve from 0.19146\n",
            "875/875 - 6s - loss: 0.1930 - accuracy: 0.9636 - val_loss: 0.1600 - val_accuracy: 0.9770 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 20/5000\n",
            "\n",
            "Epoch 20: loss did not improve from 0.19146\n",
            "875/875 - 5s - loss: 0.1937 - accuracy: 0.9621 - val_loss: 0.1649 - val_accuracy: 0.9737 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 21/5000\n",
            "\n",
            "Epoch 21: loss improved from 0.19146 to 0.18372, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.1837 - accuracy: 0.9662 - val_loss: 0.1558 - val_accuracy: 0.9763 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 22/5000\n",
            "\n",
            "Epoch 22: loss did not improve from 0.18372\n",
            "875/875 - 6s - loss: 0.1877 - accuracy: 0.9650 - val_loss: 0.1753 - val_accuracy: 0.9722 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 23/5000\n",
            "\n",
            "Epoch 23: loss did not improve from 0.18372\n",
            "875/875 - 5s - loss: 0.1842 - accuracy: 0.9661 - val_loss: 0.1965 - val_accuracy: 0.9595 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 24/5000\n",
            "\n",
            "Epoch 24: loss improved from 0.18372 to 0.18302, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1830 - accuracy: 0.9662 - val_loss: 0.1663 - val_accuracy: 0.9690 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 25/5000\n",
            "\n",
            "Epoch 25: loss did not improve from 0.18302\n",
            "875/875 - 5s - loss: 0.1864 - accuracy: 0.9664 - val_loss: 0.1500 - val_accuracy: 0.9753 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 26/5000\n",
            "\n",
            "Epoch 26: loss did not improve from 0.18302\n",
            "875/875 - 5s - loss: 0.1845 - accuracy: 0.9651 - val_loss: 0.1662 - val_accuracy: 0.9733 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 27/5000\n",
            "\n",
            "Epoch 27: loss did not improve from 0.18302\n",
            "875/875 - 5s - loss: 0.1937 - accuracy: 0.9626 - val_loss: 0.1993 - val_accuracy: 0.9493 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 28/5000\n",
            "\n",
            "Epoch 28: loss improved from 0.18302 to 0.18118, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1812 - accuracy: 0.9679 - val_loss: 0.1754 - val_accuracy: 0.9702 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 29/5000\n",
            "\n",
            "Epoch 29: loss improved from 0.18118 to 0.17970, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1797 - accuracy: 0.9662 - val_loss: 0.1523 - val_accuracy: 0.9765 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 30/5000\n",
            "\n",
            "Epoch 30: loss improved from 0.17970 to 0.17842, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.1784 - accuracy: 0.9654 - val_loss: 0.1518 - val_accuracy: 0.9757 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 31/5000\n",
            "\n",
            "Epoch 31: loss did not improve from 0.17842\n",
            "875/875 - 5s - loss: 0.1799 - accuracy: 0.9639 - val_loss: 0.1538 - val_accuracy: 0.9753 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 32/5000\n",
            "\n",
            "Epoch 32: loss did not improve from 0.17842\n",
            "875/875 - 5s - loss: 0.1900 - accuracy: 0.9639 - val_loss: 0.1651 - val_accuracy: 0.9685 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 33/5000\n",
            "\n",
            "Epoch 33: loss did not improve from 0.17842\n",
            "875/875 - 5s - loss: 0.1802 - accuracy: 0.9669 - val_loss: 0.1494 - val_accuracy: 0.9728 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 34/5000\n",
            "\n",
            "Epoch 34: loss improved from 0.17842 to 0.17686, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1769 - accuracy: 0.9645 - val_loss: 0.1685 - val_accuracy: 0.9678 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 35/5000\n",
            "\n",
            "Epoch 35: loss did not improve from 0.17686\n",
            "875/875 - 5s - loss: 0.1776 - accuracy: 0.9665 - val_loss: 0.1596 - val_accuracy: 0.9773 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 36/5000\n",
            "\n",
            "Epoch 36: loss improved from 0.17686 to 0.17448, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1745 - accuracy: 0.9656 - val_loss: 0.1544 - val_accuracy: 0.9710 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 37/5000\n",
            "\n",
            "Epoch 37: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1746 - accuracy: 0.9674 - val_loss: 0.2061 - val_accuracy: 0.9542 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 38/5000\n",
            "\n",
            "Epoch 38: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1800 - accuracy: 0.9662 - val_loss: 0.1506 - val_accuracy: 0.9747 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 39/5000\n",
            "\n",
            "Epoch 39: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1781 - accuracy: 0.9644 - val_loss: 0.1450 - val_accuracy: 0.9765 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 40/5000\n",
            "\n",
            "Epoch 40: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1768 - accuracy: 0.9659 - val_loss: 0.1502 - val_accuracy: 0.9750 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 41/5000\n",
            "\n",
            "Epoch 41: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1746 - accuracy: 0.9675 - val_loss: 0.1475 - val_accuracy: 0.9775 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 42/5000\n",
            "\n",
            "Epoch 42: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1808 - accuracy: 0.9656 - val_loss: 0.3556 - val_accuracy: 0.9102 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 43/5000\n",
            "\n",
            "Epoch 43: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1759 - accuracy: 0.9661 - val_loss: 0.1747 - val_accuracy: 0.9613 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 44/5000\n",
            "\n",
            "Epoch 44: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1754 - accuracy: 0.9669 - val_loss: 0.1610 - val_accuracy: 0.9740 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 45/5000\n",
            "\n",
            "Epoch 45: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1852 - accuracy: 0.9649 - val_loss: 0.1508 - val_accuracy: 0.9762 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 46/5000\n",
            "\n",
            "Epoch 46: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1768 - accuracy: 0.9661 - val_loss: 0.1444 - val_accuracy: 0.9745 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 47/5000\n",
            "\n",
            "Epoch 47: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1753 - accuracy: 0.9666 - val_loss: 0.1460 - val_accuracy: 0.9775 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 48/5000\n",
            "\n",
            "Epoch 48: loss did not improve from 0.17448\n",
            "875/875 - 5s - loss: 0.1805 - accuracy: 0.9643 - val_loss: 0.1597 - val_accuracy: 0.9678 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 49/5000\n",
            "\n",
            "Epoch 49: loss did not improve from 0.17448\n",
            "875/875 - 6s - loss: 0.1806 - accuracy: 0.9662 - val_loss: 0.1476 - val_accuracy: 0.9732 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 50/5000\n",
            "\n",
            "Epoch 50: loss improved from 0.17448 to 0.17285, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 7s - loss: 0.1728 - accuracy: 0.9676 - val_loss: 0.1403 - val_accuracy: 0.9768 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 51/5000\n",
            "\n",
            "Epoch 51: loss improved from 0.17285 to 0.17235, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 6s - loss: 0.1723 - accuracy: 0.9653 - val_loss: 0.1475 - val_accuracy: 0.9740 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
            "Epoch 52/5000\n",
            "\n",
            "Epoch 52: loss did not improve from 0.17235\n",
            "875/875 - 6s - loss: 0.1730 - accuracy: 0.9672 - val_loss: 0.2075 - val_accuracy: 0.9547 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 53/5000\n",
            "\n",
            "Epoch 53: loss did not improve from 0.17235\n",
            "875/875 - 5s - loss: 0.1758 - accuracy: 0.9655 - val_loss: 0.1578 - val_accuracy: 0.9740 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 54/5000\n",
            "\n",
            "Epoch 54: loss did not improve from 0.17235\n",
            "875/875 - 6s - loss: 0.1755 - accuracy: 0.9676 - val_loss: 0.1434 - val_accuracy: 0.9767 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
            "Epoch 55/5000\n",
            "\n",
            "Epoch 55: loss did not improve from 0.17235\n",
            "875/875 - 5s - loss: 0.1742 - accuracy: 0.9676 - val_loss: 0.1587 - val_accuracy: 0.9725 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 56/5000\n",
            "\n",
            "Epoch 56: loss improved from 0.17235 to 0.17112, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1711 - accuracy: 0.9673 - val_loss: 0.1424 - val_accuracy: 0.9750 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 57/5000\n",
            "\n",
            "Epoch 57: loss did not improve from 0.17112\n",
            "875/875 - 5s - loss: 0.1715 - accuracy: 0.9670 - val_loss: 0.1468 - val_accuracy: 0.9730 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 58/5000\n",
            "\n",
            "Epoch 58: loss improved from 0.17112 to 0.16844, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1684 - accuracy: 0.9672 - val_loss: 0.1424 - val_accuracy: 0.9775 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 59/5000\n",
            "\n",
            "Epoch 59: loss did not improve from 0.16844\n",
            "875/875 - 5s - loss: 0.1760 - accuracy: 0.9648 - val_loss: 0.1413 - val_accuracy: 0.9768 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 60/5000\n",
            "\n",
            "Epoch 60: loss did not improve from 0.16844\n",
            "\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "875/875 - 5s - loss: 0.1729 - accuracy: 0.9674 - val_loss: 0.2071 - val_accuracy: 0.9523 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
            "Epoch 61/5000\n",
            "\n",
            "Epoch 61: loss improved from 0.16844 to 0.15800, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1580 - accuracy: 0.9713 - val_loss: 0.1283 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 62/5000\n",
            "\n",
            "Epoch 62: loss improved from 0.15800 to 0.14849, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1485 - accuracy: 0.9716 - val_loss: 0.1266 - val_accuracy: 0.9765 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 63/5000\n",
            "\n",
            "Epoch 63: loss improved from 0.14849 to 0.14276, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1428 - accuracy: 0.9711 - val_loss: 0.1206 - val_accuracy: 0.9767 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 64/5000\n",
            "\n",
            "Epoch 64: loss improved from 0.14276 to 0.14133, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1413 - accuracy: 0.9718 - val_loss: 0.1184 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 65/5000\n",
            "\n",
            "Epoch 65: loss did not improve from 0.14133\n",
            "875/875 - 5s - loss: 0.1414 - accuracy: 0.9712 - val_loss: 0.1185 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 66/5000\n",
            "\n",
            "Epoch 66: loss improved from 0.14133 to 0.13976, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1398 - accuracy: 0.9709 - val_loss: 0.1170 - val_accuracy: 0.9758 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 67/5000\n",
            "\n",
            "Epoch 67: loss improved from 0.13976 to 0.13604, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1360 - accuracy: 0.9726 - val_loss: 0.1148 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 68/5000\n",
            "\n",
            "Epoch 68: loss did not improve from 0.13604\n",
            "875/875 - 5s - loss: 0.1377 - accuracy: 0.9710 - val_loss: 0.1151 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 69/5000\n",
            "\n",
            "Epoch 69: loss improved from 0.13604 to 0.13537, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1354 - accuracy: 0.9736 - val_loss: 0.1149 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 70/5000\n",
            "\n",
            "Epoch 70: loss improved from 0.13537 to 0.13298, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1330 - accuracy: 0.9728 - val_loss: 0.1144 - val_accuracy: 0.9765 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 71/5000\n",
            "\n",
            "Epoch 71: loss did not improve from 0.13298\n",
            "875/875 - 5s - loss: 0.1364 - accuracy: 0.9709 - val_loss: 0.1132 - val_accuracy: 0.9765 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 72/5000\n",
            "\n",
            "Epoch 72: loss did not improve from 0.13298\n",
            "875/875 - 5s - loss: 0.1352 - accuracy: 0.9726 - val_loss: 0.1134 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 73/5000\n",
            "\n",
            "Epoch 73: loss did not improve from 0.13298\n",
            "875/875 - 5s - loss: 0.1337 - accuracy: 0.9729 - val_loss: 0.1118 - val_accuracy: 0.9785 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 74/5000\n",
            "\n",
            "Epoch 74: loss did not improve from 0.13298\n",
            "875/875 - 5s - loss: 0.1360 - accuracy: 0.9700 - val_loss: 0.1130 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 75/5000\n",
            "\n",
            "Epoch 75: loss did not improve from 0.13298\n",
            "875/875 - 5s - loss: 0.1332 - accuracy: 0.9728 - val_loss: 0.1121 - val_accuracy: 0.9758 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 76/5000\n",
            "\n",
            "Epoch 76: loss did not improve from 0.13298\n",
            "875/875 - 5s - loss: 0.1341 - accuracy: 0.9709 - val_loss: 0.1129 - val_accuracy: 0.9760 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 77/5000\n",
            "\n",
            "Epoch 77: loss improved from 0.13298 to 0.13283, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1328 - accuracy: 0.9728 - val_loss: 0.1124 - val_accuracy: 0.9762 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 78/5000\n",
            "\n",
            "Epoch 78: loss improved from 0.13283 to 0.13229, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1323 - accuracy: 0.9707 - val_loss: 0.1121 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 79/5000\n",
            "\n",
            "Epoch 79: loss improved from 0.13229 to 0.13099, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1310 - accuracy: 0.9723 - val_loss: 0.1122 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 80/5000\n",
            "\n",
            "Epoch 80: loss did not improve from 0.13099\n",
            "875/875 - 5s - loss: 0.1367 - accuracy: 0.9717 - val_loss: 0.1117 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 81/5000\n",
            "\n",
            "Epoch 81: loss improved from 0.13099 to 0.13004, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1300 - accuracy: 0.9724 - val_loss: 0.1114 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 82/5000\n",
            "\n",
            "Epoch 82: loss did not improve from 0.13004\n",
            "875/875 - 5s - loss: 0.1338 - accuracy: 0.9714 - val_loss: 0.1117 - val_accuracy: 0.9782 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 83/5000\n",
            "\n",
            "Epoch 83: loss did not improve from 0.13004\n",
            "875/875 - 5s - loss: 0.1308 - accuracy: 0.9727 - val_loss: 0.1111 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 84/5000\n",
            "\n",
            "Epoch 84: loss did not improve from 0.13004\n",
            "875/875 - 5s - loss: 0.1329 - accuracy: 0.9714 - val_loss: 0.1101 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 85/5000\n",
            "\n",
            "Epoch 85: loss did not improve from 0.13004\n",
            "875/875 - 5s - loss: 0.1313 - accuracy: 0.9718 - val_loss: 0.1097 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 86/5000\n",
            "\n",
            "Epoch 86: loss improved from 0.13004 to 0.12951, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1295 - accuracy: 0.9724 - val_loss: 0.1091 - val_accuracy: 0.9782 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 87/5000\n",
            "\n",
            "Epoch 87: loss did not improve from 0.12951\n",
            "875/875 - 5s - loss: 0.1312 - accuracy: 0.9706 - val_loss: 0.1092 - val_accuracy: 0.9760 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 88/5000\n",
            "\n",
            "Epoch 88: loss did not improve from 0.12951\n",
            "875/875 - 5s - loss: 0.1305 - accuracy: 0.9714 - val_loss: 0.1123 - val_accuracy: 0.9782 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 89/5000\n",
            "\n",
            "Epoch 89: loss improved from 0.12951 to 0.12929, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1293 - accuracy: 0.9719 - val_loss: 0.1082 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 90/5000\n",
            "\n",
            "Epoch 90: loss did not improve from 0.12929\n",
            "875/875 - 5s - loss: 0.1301 - accuracy: 0.9713 - val_loss: 0.1104 - val_accuracy: 0.9763 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 91/5000\n",
            "\n",
            "Epoch 91: loss improved from 0.12929 to 0.12885, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1289 - accuracy: 0.9725 - val_loss: 0.1092 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 92/5000\n",
            "\n",
            "Epoch 92: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1295 - accuracy: 0.9714 - val_loss: 0.1085 - val_accuracy: 0.9767 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 93/5000\n",
            "\n",
            "Epoch 93: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1304 - accuracy: 0.9709 - val_loss: 0.1075 - val_accuracy: 0.9770 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 94/5000\n",
            "\n",
            "Epoch 94: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1290 - accuracy: 0.9725 - val_loss: 0.1096 - val_accuracy: 0.9763 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 95/5000\n",
            "\n",
            "Epoch 95: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1317 - accuracy: 0.9711 - val_loss: 0.1092 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 96/5000\n",
            "\n",
            "Epoch 96: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1291 - accuracy: 0.9723 - val_loss: 0.1091 - val_accuracy: 0.9770 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 97/5000\n",
            "\n",
            "Epoch 97: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1301 - accuracy: 0.9724 - val_loss: 0.1087 - val_accuracy: 0.9783 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 98/5000\n",
            "\n",
            "Epoch 98: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1300 - accuracy: 0.9713 - val_loss: 0.1100 - val_accuracy: 0.9785 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 99/5000\n",
            "\n",
            "Epoch 99: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1291 - accuracy: 0.9716 - val_loss: 0.1067 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 100/5000\n",
            "\n",
            "Epoch 100: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1296 - accuracy: 0.9720 - val_loss: 0.1070 - val_accuracy: 0.9787 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 101/5000\n",
            "\n",
            "Epoch 101: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1291 - accuracy: 0.9714 - val_loss: 0.1074 - val_accuracy: 0.9765 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 102/5000\n",
            "\n",
            "Epoch 102: loss did not improve from 0.12885\n",
            "875/875 - 5s - loss: 0.1291 - accuracy: 0.9712 - val_loss: 0.1065 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 103/5000\n",
            "\n",
            "Epoch 103: loss improved from 0.12885 to 0.12812, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1281 - accuracy: 0.9719 - val_loss: 0.1083 - val_accuracy: 0.9768 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 104/5000\n",
            "\n",
            "Epoch 104: loss did not improve from 0.12812\n",
            "875/875 - 5s - loss: 0.1313 - accuracy: 0.9720 - val_loss: 0.1110 - val_accuracy: 0.9770 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 105/5000\n",
            "\n",
            "Epoch 105: loss did not improve from 0.12812\n",
            "875/875 - 5s - loss: 0.1296 - accuracy: 0.9716 - val_loss: 0.1086 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 106/5000\n",
            "\n",
            "Epoch 106: loss improved from 0.12812 to 0.12778, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1278 - accuracy: 0.9708 - val_loss: 0.1093 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 107/5000\n",
            "\n",
            "Epoch 107: loss improved from 0.12778 to 0.12683, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1268 - accuracy: 0.9714 - val_loss: 0.1057 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 108/5000\n",
            "\n",
            "Epoch 108: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1285 - accuracy: 0.9713 - val_loss: 0.1069 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 109/5000\n",
            "\n",
            "Epoch 109: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1269 - accuracy: 0.9721 - val_loss: 0.1082 - val_accuracy: 0.9750 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 110/5000\n",
            "\n",
            "Epoch 110: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1275 - accuracy: 0.9719 - val_loss: 0.1076 - val_accuracy: 0.9770 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 111/5000\n",
            "\n",
            "Epoch 111: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1273 - accuracy: 0.9716 - val_loss: 0.1079 - val_accuracy: 0.9755 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 112/5000\n",
            "\n",
            "Epoch 112: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1309 - accuracy: 0.9696 - val_loss: 0.1076 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 113/5000\n",
            "\n",
            "Epoch 113: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1270 - accuracy: 0.9724 - val_loss: 0.1095 - val_accuracy: 0.9745 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 114/5000\n",
            "\n",
            "Epoch 114: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1292 - accuracy: 0.9719 - val_loss: 0.1085 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 115/5000\n",
            "\n",
            "Epoch 115: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1271 - accuracy: 0.9715 - val_loss: 0.1082 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 116/5000\n",
            "\n",
            "Epoch 116: loss did not improve from 0.12683\n",
            "875/875 - 5s - loss: 0.1307 - accuracy: 0.9705 - val_loss: 0.1078 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 117/5000\n",
            "\n",
            "Epoch 117: loss did not improve from 0.12683\n",
            "\n",
            "Epoch 117: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "875/875 - 5s - loss: 0.1283 - accuracy: 0.9712 - val_loss: 0.1062 - val_accuracy: 0.9778 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 118/5000\n",
            "\n",
            "Epoch 118: loss improved from 0.12683 to 0.12563, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1256 - accuracy: 0.9729 - val_loss: 0.1062 - val_accuracy: 0.9737 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 119/5000\n",
            "\n",
            "Epoch 119: loss did not improve from 0.12563\n",
            "875/875 - 5s - loss: 0.1286 - accuracy: 0.9714 - val_loss: 0.1066 - val_accuracy: 0.9767 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 120/5000\n",
            "\n",
            "Epoch 120: loss did not improve from 0.12563\n",
            "875/875 - 5s - loss: 0.1287 - accuracy: 0.9723 - val_loss: 0.1062 - val_accuracy: 0.9780 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 121/5000\n",
            "\n",
            "Epoch 121: loss did not improve from 0.12563\n",
            "875/875 - 5s - loss: 0.1262 - accuracy: 0.9729 - val_loss: 0.1081 - val_accuracy: 0.9758 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 122/5000\n",
            "\n",
            "Epoch 122: loss did not improve from 0.12563\n",
            "875/875 - 5s - loss: 0.1267 - accuracy: 0.9726 - val_loss: 0.1054 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 123/5000\n",
            "\n",
            "Epoch 123: loss did not improve from 0.12563\n",
            "875/875 - 5s - loss: 0.1297 - accuracy: 0.9716 - val_loss: 0.1085 - val_accuracy: 0.9782 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 124/5000\n",
            "\n",
            "Epoch 124: loss improved from 0.12563 to 0.12560, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1256 - accuracy: 0.9735 - val_loss: 0.1072 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 125/5000\n",
            "\n",
            "Epoch 125: loss did not improve from 0.12560\n",
            "875/875 - 5s - loss: 0.1261 - accuracy: 0.9727 - val_loss: 0.1081 - val_accuracy: 0.9768 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 126/5000\n",
            "\n",
            "Epoch 126: loss improved from 0.12560 to 0.12453, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 5s - loss: 0.1245 - accuracy: 0.9724 - val_loss: 0.1055 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 127/5000\n",
            "\n",
            "Epoch 127: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1270 - accuracy: 0.9714 - val_loss: 0.1060 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 128/5000\n",
            "\n",
            "Epoch 128: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1266 - accuracy: 0.9724 - val_loss: 0.1042 - val_accuracy: 0.9773 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 129/5000\n",
            "\n",
            "Epoch 129: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1285 - accuracy: 0.9726 - val_loss: 0.1044 - val_accuracy: 0.9770 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 130/5000\n",
            "\n",
            "Epoch 130: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1307 - accuracy: 0.9706 - val_loss: 0.1079 - val_accuracy: 0.9772 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 131/5000\n",
            "\n",
            "Epoch 131: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1273 - accuracy: 0.9719 - val_loss: 0.1061 - val_accuracy: 0.9775 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 132/5000\n",
            "\n",
            "Epoch 132: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1293 - accuracy: 0.9719 - val_loss: 0.1122 - val_accuracy: 0.9750 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 133/5000\n",
            "\n",
            "Epoch 133: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1307 - accuracy: 0.9708 - val_loss: 0.1056 - val_accuracy: 0.9777 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 134/5000\n",
            "\n",
            "Epoch 134: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1270 - accuracy: 0.9718 - val_loss: 0.1050 - val_accuracy: 0.9747 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 135/5000\n",
            "\n",
            "Epoch 135: loss did not improve from 0.12453\n",
            "875/875 - 5s - loss: 0.1254 - accuracy: 0.9726 - val_loss: 0.1054 - val_accuracy: 0.9745 - lr: 1.0000e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 136/5000\n",
            "\n",
            "Epoch 136: loss did not improve from 0.12453\n",
            "875/875 - 4861s - loss: 0.1288 - accuracy: 0.9711 - val_loss: 0.1047 - val_accuracy: 0.9747 - lr: 1.0000e-04 - 4861s/epoch - 6s/step\n",
            "Epoch 137/5000\n",
            "\n",
            "Epoch 137: loss improved from 0.12453 to 0.12371, saving model to model_LTE_WiFi_coexistance_histogram.h5\n",
            "875/875 - 12s - loss: 0.1237 - accuracy: 0.9720 - val_loss: 0.1051 - val_accuracy: 0.9788 - lr: 1.0000e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 138/5000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m nb_epoch \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m      6\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m----> 7\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x \u001b[39m=\u001b[39;49m [X_train, Y_train], y \u001b[39m=\u001b[39;49m Label_train ,\n\u001b[0;32m      8\u001b[0m                     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      9\u001b[0m                     epochs\u001b[39m=\u001b[39;49mnb_epoch,\n\u001b[0;32m     10\u001b[0m                     validation_data \u001b[39m=\u001b[39;49m ([X_test, Y_test], Label_test),\n\u001b[0;32m     11\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     12\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m                     callbacks \u001b[39m=\u001b[39;49m [early_stop, checkpoint, reduce_lr_callback] )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "savedir = 'model_LTE_WiFi_coexistance_histogram.h5'\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'loss', min_delta = 0, patience = 20, verbose = 0, mode = 'auto')\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(savedir, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_delta=0.00001, cooldown=0, min_lr=0.0001)\n",
        "nb_epoch = 5000\n",
        "batch_size = 16\n",
        "history = model.fit(x = [X_train, Y_train], y = Label_train ,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=nb_epoch,\n",
        "                    validation_data = ([X_test, Y_test], Label_test),\n",
        "                    shuffle=True,\n",
        "                    verbose=2,\n",
        "                    callbacks = [early_stop, checkpoint, reduce_lr_callback] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ouA10fBUkGaK",
        "outputId": "35242829-7a87-42b3-8ca3-c1837cbbfca0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nirBsGSOuei4",
        "outputId": "ab084848-a10a-48a9-9c43-c69bd4f02175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
          ]
        }
      ],
      "source": [
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bHk1-opLukvn",
        "outputId": "63d84b67-5617-4838-ce6e-90e237bd11ca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2klqx3yu2W9"
      },
      "source": [
        "**Plot Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNHkLmKZunnU"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=45)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "tSyWlUZgu7ID",
        "outputId": "7badb689-1535-4a7a-f96d-355220a05891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 2s 3ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "X = [X_test, Y_test]\n",
        "\n",
        "test_Y_hat = model.predict(X, batch_size=batch_size)\n",
        "conf = np.zeros([len(classes),len(classes)])\n",
        "confnorm1 = np.zeros([len(classes),len(classes)])\n",
        "for i in range(0,X_test.shape[0]):\n",
        "    j = list(Label_test[i,:]).index(1)\n",
        "    k = int(np.argmax(test_Y_hat[i,:]))\n",
        "    conf[j,k] = conf[j,k] + 1\n",
        "for i in range(0,len(classes)):\n",
        "    confnorm1[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
        "plot_confusion_matrix(confnorm1, labels=classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGudgdG2vCSe",
        "outputId": "c5dc348a-2fce-438d-c677-077ab2bedb5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.98735441 0.01264559]\n",
            " [0.03873122 0.96126878]]\n"
          ]
        }
      ],
      "source": [
        "print(confnorm1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q25lNPGjvF6X",
        "outputId": "4a12edb7-bc15-488a-deb0-a0314802de37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - 2s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict([X_test, Y_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF7HquqpvMMB"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq2si5WgvQ3n"
      },
      "outputs": [],
      "source": [
        "Label_test = np.argmax(Label_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek5n2F63vTjb",
        "outputId": "67122ddf-e21c-42d9-8c69-bab3a6a467ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9743333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(Label_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWyfFk1XvWyk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2GF4qTeveUJ"
      },
      "outputs": [],
      "source": [
        "model = load_model('model_LTE_WiFi_coexistance_histogram.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JytnpJfYvisA"
      },
      "outputs": [],
      "source": [
        "batch_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QER6WwGxvlbJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"generality.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ZbB0LW4vn-0",
        "outputId": "bee6447c-6dbe-4fe5-e66f-f64bc002bcb9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-17a9fd6e-395d-4dd7-a3ed-d6383ebc5362\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0339</td>\n",
              "      <td>0.0677</td>\n",
              "      <td>0.1016</td>\n",
              "      <td>0.1355</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.2371</td>\n",
              "      <td>0.2710</td>\n",
              "      <td>0.3049</td>\n",
              "      <td>0.3387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>0.7891</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.0478</td>\n",
              "      <td>0.0637</td>\n",
              "      <td>0.0796</td>\n",
              "      <td>0.0956</td>\n",
              "      <td>0.1115</td>\n",
              "      <td>0.1274</td>\n",
              "      <td>0.1434</td>\n",
              "      <td>0.1593</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.8106</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0128</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.0256</td>\n",
              "      <td>0.0320</td>\n",
              "      <td>0.0384</td>\n",
              "      <td>0.0448</td>\n",
              "      <td>0.0512</td>\n",
              "      <td>0.0576</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0459</td>\n",
              "      <td>8732.0000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0246</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.0411</td>\n",
              "      <td>0.0493</td>\n",
              "      <td>0.0575</td>\n",
              "      <td>0.0657</td>\n",
              "      <td>0.0739</td>\n",
              "      <td>0.0821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0446</td>\n",
              "      <td>0.7188</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0190</td>\n",
              "      <td>0.0285</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>0.0475</td>\n",
              "      <td>0.0570</td>\n",
              "      <td>0.0665</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0855</td>\n",
              "      <td>0.0950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.8626</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0327</td>\n",
              "      <td>0.0654</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1308</td>\n",
              "      <td>0.1635</td>\n",
              "      <td>0.1963</td>\n",
              "      <td>0.2290</td>\n",
              "      <td>0.2617</td>\n",
              "      <td>0.2944</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0418</td>\n",
              "      <td>0.9692</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0487</td>\n",
              "      <td>0.0973</td>\n",
              "      <td>0.1460</td>\n",
              "      <td>0.1947</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>0.3407</td>\n",
              "      <td>0.3893</td>\n",
              "      <td>0.4380</td>\n",
              "      <td>0.4867</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0429</td>\n",
              "      <td>0.9495</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0206</td>\n",
              "      <td>0.0412</td>\n",
              "      <td>0.0618</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>0.1031</td>\n",
              "      <td>0.1237</td>\n",
              "      <td>0.1443</td>\n",
              "      <td>0.1649</td>\n",
              "      <td>0.1855</td>\n",
              "      <td>0.2062</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0425</td>\n",
              "      <td>0.8598</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0297</td>\n",
              "      <td>0.0445</td>\n",
              "      <td>0.0593</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0890</td>\n",
              "      <td>0.1038</td>\n",
              "      <td>0.1187</td>\n",
              "      <td>0.1335</td>\n",
              "      <td>0.1483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.8890</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0204</td>\n",
              "      <td>0.0407</td>\n",
              "      <td>0.0611</td>\n",
              "      <td>0.0815</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1222</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.2037</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0455</td>\n",
              "      <td>0.7850</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0888</td>\n",
              "      <td>0.1776</td>\n",
              "      <td>0.2665</td>\n",
              "      <td>0.3553</td>\n",
              "      <td>0.4441</td>\n",
              "      <td>0.5329</td>\n",
              "      <td>0.6217</td>\n",
              "      <td>0.7106</td>\n",
              "      <td>0.7994</td>\n",
              "      <td>0.8882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.9815</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0506</td>\n",
              "      <td>0.1012</td>\n",
              "      <td>0.1518</td>\n",
              "      <td>0.2024</td>\n",
              "      <td>0.2530</td>\n",
              "      <td>0.3036</td>\n",
              "      <td>0.3542</td>\n",
              "      <td>0.4048</td>\n",
              "      <td>0.4554</td>\n",
              "      <td>0.5060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0898</td>\n",
              "      <td>0.9714</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0875</td>\n",
              "      <td>0.1750</td>\n",
              "      <td>0.2625</td>\n",
              "      <td>0.3500</td>\n",
              "      <td>0.4376</td>\n",
              "      <td>0.5251</td>\n",
              "      <td>0.6126</td>\n",
              "      <td>0.7001</td>\n",
              "      <td>0.7876</td>\n",
              "      <td>0.8751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.1550</td>\n",
              "      <td>0.9829</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0224</td>\n",
              "      <td>0.0447</td>\n",
              "      <td>0.0671</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.1119</td>\n",
              "      <td>0.1342</td>\n",
              "      <td>0.1566</td>\n",
              "      <td>0.1790</td>\n",
              "      <td>0.2014</td>\n",
              "      <td>0.2237</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.7446</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0463</td>\n",
              "      <td>0.0695</td>\n",
              "      <td>0.0926</td>\n",
              "      <td>0.1158</td>\n",
              "      <td>0.1389</td>\n",
              "      <td>0.1621</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>0.2084</td>\n",
              "      <td>0.2316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0450</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0208</td>\n",
              "      <td>0.0416</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0831</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>0.1455</td>\n",
              "      <td>0.1662</td>\n",
              "      <td>0.1870</td>\n",
              "      <td>0.2078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0401</td>\n",
              "      <td>0.0601</td>\n",
              "      <td>0.0802</td>\n",
              "      <td>0.1002</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1603</td>\n",
              "      <td>0.1803</td>\n",
              "      <td>0.2004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.9248</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0208</td>\n",
              "      <td>0.0415</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0831</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>0.1246</td>\n",
              "      <td>0.1454</td>\n",
              "      <td>0.1662</td>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.2077</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0560</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0504</td>\n",
              "      <td>0.5776</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0818</td>\n",
              "      <td>0.1227</td>\n",
              "      <td>0.1637</td>\n",
              "      <td>0.2046</td>\n",
              "      <td>0.2455</td>\n",
              "      <td>0.2864</td>\n",
              "      <td>0.3273</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.4091</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0114</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0496</td>\n",
              "      <td>0.5919</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0340</td>\n",
              "      <td>0.0680</td>\n",
              "      <td>0.1021</td>\n",
              "      <td>0.1361</td>\n",
              "      <td>0.1701</td>\n",
              "      <td>0.2041</td>\n",
              "      <td>0.2381</td>\n",
              "      <td>0.2722</td>\n",
              "      <td>0.3062</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.7065</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0450</td>\n",
              "      <td>0.0540</td>\n",
              "      <td>0.0629</td>\n",
              "      <td>0.0719</td>\n",
              "      <td>0.0809</td>\n",
              "      <td>0.0899</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0450</td>\n",
              "      <td>0.7215</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0211</td>\n",
              "      <td>0.0422</td>\n",
              "      <td>0.0633</td>\n",
              "      <td>0.0844</td>\n",
              "      <td>0.1055</td>\n",
              "      <td>0.1266</td>\n",
              "      <td>0.1477</td>\n",
              "      <td>0.1689</td>\n",
              "      <td>0.1900</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0455</td>\n",
              "      <td>0.7425</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0343</td>\n",
              "      <td>0.0514</td>\n",
              "      <td>0.0686</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>0.1028</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1371</td>\n",
              "      <td>0.1542</td>\n",
              "      <td>0.1714</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.7370</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0297</td>\n",
              "      <td>0.0594</td>\n",
              "      <td>0.0891</td>\n",
              "      <td>0.1188</td>\n",
              "      <td>0.1484</td>\n",
              "      <td>0.1781</td>\n",
              "      <td>0.2078</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.2672</td>\n",
              "      <td>0.2969</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0463</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.1189</td>\n",
              "      <td>0.2377</td>\n",
              "      <td>0.3566</td>\n",
              "      <td>0.4754</td>\n",
              "      <td>0.5943</td>\n",
              "      <td>0.7131</td>\n",
              "      <td>0.8320</td>\n",
              "      <td>0.9508</td>\n",
              "      <td>1.0697</td>\n",
              "      <td>1.1885</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.9917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0356</td>\n",
              "      <td>0.0713</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>0.1782</td>\n",
              "      <td>0.2138</td>\n",
              "      <td>0.2495</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>0.3208</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0295</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.1331</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.9859</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0537</td>\n",
              "      <td>0.1073</td>\n",
              "      <td>0.1610</td>\n",
              "      <td>0.2146</td>\n",
              "      <td>0.2683</td>\n",
              "      <td>0.3219</td>\n",
              "      <td>0.3756</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.4829</td>\n",
              "      <td>0.5366</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.0300</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.1152</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.9884</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0715</td>\n",
              "      <td>0.1072</td>\n",
              "      <td>0.1429</td>\n",
              "      <td>0.1786</td>\n",
              "      <td>0.2144</td>\n",
              "      <td>0.2501</td>\n",
              "      <td>0.2858</td>\n",
              "      <td>0.3216</td>\n",
              "      <td>0.3573</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0404</td>\n",
              "      <td>0.0432</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.1565</td>\n",
              "      <td>0.9881</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0644</td>\n",
              "      <td>0.1287</td>\n",
              "      <td>0.1931</td>\n",
              "      <td>0.2574</td>\n",
              "      <td>0.3218</td>\n",
              "      <td>0.3862</td>\n",
              "      <td>0.4505</td>\n",
              "      <td>0.5149</td>\n",
              "      <td>0.5792</td>\n",
              "      <td>0.6436</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0430</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.1565</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0299</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.0897</td>\n",
              "      <td>0.1196</td>\n",
              "      <td>0.1495</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.2093</td>\n",
              "      <td>0.2392</td>\n",
              "      <td>0.2691</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0400</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0720</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.1567</td>\n",
              "      <td>0.9835</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.0307</td>\n",
              "      <td>0.0614</td>\n",
              "      <td>0.0921</td>\n",
              "      <td>0.1228</td>\n",
              "      <td>0.1535</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.2149</td>\n",
              "      <td>0.2456</td>\n",
              "      <td>0.2763</td>\n",
              "      <td>0.3070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0396</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>0.0370</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.1574</td>\n",
              "      <td>0.9905</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.0658</td>\n",
              "      <td>0.1316</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.2632</td>\n",
              "      <td>0.3290</td>\n",
              "      <td>0.3948</td>\n",
              "      <td>0.4606</td>\n",
              "      <td>0.5264</td>\n",
              "      <td>0.5922</td>\n",
              "      <td>0.6580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0944</td>\n",
              "      <td>0.1337</td>\n",
              "      <td>0.9854</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.1626</td>\n",
              "      <td>0.3252</td>\n",
              "      <td>0.4879</td>\n",
              "      <td>0.6505</td>\n",
              "      <td>0.8131</td>\n",
              "      <td>0.9757</td>\n",
              "      <td>1.1383</td>\n",
              "      <td>1.3010</td>\n",
              "      <td>1.4636</td>\n",
              "      <td>1.6262</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0216</td>\n",
              "      <td>1.5060</td>\n",
              "      <td>0.9852</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.0036</td>\n",
              "      <td>2.0072</td>\n",
              "      <td>3.0109</td>\n",
              "      <td>4.0145</td>\n",
              "      <td>5.0181</td>\n",
              "      <td>6.0217</td>\n",
              "      <td>7.0254</td>\n",
              "      <td>8.0290</td>\n",
              "      <td>9.0326</td>\n",
              "      <td>10.0360</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>1.7322</td>\n",
              "      <td>0.9846</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.0055</td>\n",
              "      <td>2.0109</td>\n",
              "      <td>3.0164</td>\n",
              "      <td>4.0218</td>\n",
              "      <td>5.0273</td>\n",
              "      <td>6.0328</td>\n",
              "      <td>7.0382</td>\n",
              "      <td>8.0437</td>\n",
              "      <td>9.0492</td>\n",
              "      <td>10.0550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>1.6474</td>\n",
              "      <td>0.9865</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1.0018</td>\n",
              "      <td>2.0035</td>\n",
              "      <td>3.0053</td>\n",
              "      <td>4.0070</td>\n",
              "      <td>5.0088</td>\n",
              "      <td>6.0105</td>\n",
              "      <td>7.0123</td>\n",
              "      <td>8.0140</td>\n",
              "      <td>9.0158</td>\n",
              "      <td>10.0180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>1.5185</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.4191</td>\n",
              "      <td>0.8382</td>\n",
              "      <td>1.2573</td>\n",
              "      <td>1.6763</td>\n",
              "      <td>2.0954</td>\n",
              "      <td>2.5145</td>\n",
              "      <td>2.9336</td>\n",
              "      <td>3.3527</td>\n",
              "      <td>3.7718</td>\n",
              "      <td>4.1909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.8517</td>\n",
              "      <td>0.9909</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.5667</td>\n",
              "      <td>1.1335</td>\n",
              "      <td>1.7002</td>\n",
              "      <td>2.2670</td>\n",
              "      <td>2.8337</td>\n",
              "      <td>3.4005</td>\n",
              "      <td>3.9672</td>\n",
              "      <td>4.5340</td>\n",
              "      <td>5.1007</td>\n",
              "      <td>5.6675</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.5892</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.0940</td>\n",
              "      <td>0.1881</td>\n",
              "      <td>0.2821</td>\n",
              "      <td>0.3762</td>\n",
              "      <td>0.4702</td>\n",
              "      <td>0.5643</td>\n",
              "      <td>0.6583</td>\n",
              "      <td>0.7524</td>\n",
              "      <td>0.8464</td>\n",
              "      <td>0.9405</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.1507</td>\n",
              "      <td>0.9732</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0662</td>\n",
              "      <td>0.1325</td>\n",
              "      <td>0.1987</td>\n",
              "      <td>0.2649</td>\n",
              "      <td>0.3311</td>\n",
              "      <td>0.3974</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.5298</td>\n",
              "      <td>0.5960</td>\n",
              "      <td>0.6623</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.9798</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.0757</td>\n",
              "      <td>0.1515</td>\n",
              "      <td>0.2272</td>\n",
              "      <td>0.3030</td>\n",
              "      <td>0.3787</td>\n",
              "      <td>0.4545</td>\n",
              "      <td>0.5302</td>\n",
              "      <td>0.6059</td>\n",
              "      <td>0.6817</td>\n",
              "      <td>0.7574</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0301</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.9831</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.1076</td>\n",
              "      <td>0.1615</td>\n",
              "      <td>0.2153</td>\n",
              "      <td>0.2691</td>\n",
              "      <td>0.3229</td>\n",
              "      <td>0.3767</td>\n",
              "      <td>0.4305</td>\n",
              "      <td>0.4844</td>\n",
              "      <td>0.5382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1448</td>\n",
              "      <td>0.9893</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.3429</td>\n",
              "      <td>0.5144</td>\n",
              "      <td>0.6858</td>\n",
              "      <td>0.8573</td>\n",
              "      <td>1.0287</td>\n",
              "      <td>1.2002</td>\n",
              "      <td>1.3716</td>\n",
              "      <td>1.5431</td>\n",
              "      <td>1.7146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.6818</td>\n",
              "      <td>0.9870</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.1727</td>\n",
              "      <td>0.3455</td>\n",
              "      <td>0.5182</td>\n",
              "      <td>0.6909</td>\n",
              "      <td>0.8637</td>\n",
              "      <td>1.0364</td>\n",
              "      <td>1.2091</td>\n",
              "      <td>1.3819</td>\n",
              "      <td>1.5546</td>\n",
              "      <td>1.7273</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0532</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.6818</td>\n",
              "      <td>0.9880</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.1854</td>\n",
              "      <td>0.3708</td>\n",
              "      <td>0.5562</td>\n",
              "      <td>0.7416</td>\n",
              "      <td>0.9269</td>\n",
              "      <td>1.1123</td>\n",
              "      <td>1.2977</td>\n",
              "      <td>1.4831</td>\n",
              "      <td>1.6685</td>\n",
              "      <td>1.8539</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0545</td>\n",
              "      <td>0.6405</td>\n",
              "      <td>0.9950</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.3698</td>\n",
              "      <td>0.7395</td>\n",
              "      <td>1.1093</td>\n",
              "      <td>1.4791</td>\n",
              "      <td>1.8488</td>\n",
              "      <td>2.2186</td>\n",
              "      <td>2.5884</td>\n",
              "      <td>2.9581</td>\n",
              "      <td>3.3279</td>\n",
              "      <td>3.6977</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.3057</td>\n",
              "      <td>0.9772</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.3841</td>\n",
              "      <td>0.7681</td>\n",
              "      <td>1.1522</td>\n",
              "      <td>1.5362</td>\n",
              "      <td>1.9203</td>\n",
              "      <td>2.3043</td>\n",
              "      <td>2.6884</td>\n",
              "      <td>3.0724</td>\n",
              "      <td>3.4565</td>\n",
              "      <td>3.8405</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.3056</td>\n",
              "      <td>0.9537</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.3738</td>\n",
              "      <td>0.5607</td>\n",
              "      <td>0.7476</td>\n",
              "      <td>0.9345</td>\n",
              "      <td>1.1214</td>\n",
              "      <td>1.3083</td>\n",
              "      <td>1.4952</td>\n",
              "      <td>1.6821</td>\n",
              "      <td>1.8690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0733</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.4535</td>\n",
              "      <td>0.9815</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.3240</td>\n",
              "      <td>0.6480</td>\n",
              "      <td>0.9721</td>\n",
              "      <td>1.2961</td>\n",
              "      <td>1.6201</td>\n",
              "      <td>1.9441</td>\n",
              "      <td>2.2681</td>\n",
              "      <td>2.5922</td>\n",
              "      <td>2.9162</td>\n",
              "      <td>3.2402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.9891</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.4862</td>\n",
              "      <td>0.7293</td>\n",
              "      <td>0.9724</td>\n",
              "      <td>1.2155</td>\n",
              "      <td>1.4586</td>\n",
              "      <td>1.7017</td>\n",
              "      <td>1.9448</td>\n",
              "      <td>2.1879</td>\n",
              "      <td>2.4310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.2224</td>\n",
              "      <td>0.9862</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51 rows × 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17a9fd6e-395d-4dd7-a3ed-d6383ebc5362')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17a9fd6e-395d-4dd7-a3ed-d6383ebc5362 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17a9fd6e-395d-4dd7-a3ed-d6383ebc5362');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        0       1       2       3       4       5       6       7       8   \\\n",
              "0   0.0339  0.0677  0.1016  0.1355  0.1694  0.2032  0.2371  0.2710  0.3049   \n",
              "1   0.0159  0.0319  0.0478  0.0637  0.0796  0.0956  0.1115  0.1274  0.1434   \n",
              "2   0.0064  0.0128  0.0192  0.0256  0.0320  0.0384  0.0448  0.0512  0.0576   \n",
              "3   0.0082  0.0164  0.0246  0.0328  0.0411  0.0493  0.0575  0.0657  0.0739   \n",
              "4   0.0095  0.0190  0.0285  0.0380  0.0475  0.0570  0.0665  0.0760  0.0855   \n",
              "5   0.0327  0.0654  0.0981  0.1308  0.1635  0.1963  0.2290  0.2617  0.2944   \n",
              "6   0.0487  0.0973  0.1460  0.1947  0.2433  0.2920  0.3407  0.3893  0.4380   \n",
              "7   0.0206  0.0412  0.0618  0.0825  0.1031  0.1237  0.1443  0.1649  0.1855   \n",
              "8   0.0148  0.0297  0.0445  0.0593  0.0742  0.0890  0.1038  0.1187  0.1335   \n",
              "9   0.0204  0.0407  0.0611  0.0815  0.1018  0.1222  0.1426  0.1630  0.1833   \n",
              "10  0.0888  0.1776  0.2665  0.3553  0.4441  0.5329  0.6217  0.7106  0.7994   \n",
              "11  0.0506  0.1012  0.1518  0.2024  0.2530  0.3036  0.3542  0.4048  0.4554   \n",
              "12  0.0875  0.1750  0.2625  0.3500  0.4376  0.5251  0.6126  0.7001  0.7876   \n",
              "13  0.0224  0.0447  0.0671  0.0895  0.1119  0.1342  0.1566  0.1790  0.2014   \n",
              "14  0.0232  0.0463  0.0695  0.0926  0.1158  0.1389  0.1621  0.1853  0.2084   \n",
              "15  0.0208  0.0416  0.0623  0.0831  0.1039  0.1247  0.1455  0.1662  0.1870   \n",
              "16  0.0200  0.0401  0.0601  0.0802  0.1002  0.1202  0.1403  0.1603  0.1803   \n",
              "17  0.0208  0.0415  0.0623  0.0831  0.1039  0.1246  0.1454  0.1662  0.1869   \n",
              "18  0.0409  0.0818  0.1227  0.1637  0.2046  0.2455  0.2864  0.3273  0.3682   \n",
              "19  0.0340  0.0680  0.1021  0.1361  0.1701  0.2041  0.2381  0.2722  0.3062   \n",
              "20  0.0090  0.0180  0.0270  0.0360  0.0450  0.0540  0.0629  0.0719  0.0809   \n",
              "21  0.0211  0.0422  0.0633  0.0844  0.1055  0.1266  0.1477  0.1689  0.1900   \n",
              "22  0.0171  0.0343  0.0514  0.0686  0.0857  0.1028  0.1200  0.1371  0.1542   \n",
              "23  0.0297  0.0594  0.0891  0.1188  0.1484  0.1781  0.2078  0.2375  0.2672   \n",
              "24  0.1189  0.2377  0.3566  0.4754  0.5943  0.7131  0.8320  0.9508  1.0697   \n",
              "25  0.0356  0.0713  0.1069  0.1426  0.1782  0.2138  0.2495  0.2851  0.3208   \n",
              "26  0.0537  0.1073  0.1610  0.2146  0.2683  0.3219  0.3756  0.4293  0.4829   \n",
              "27  0.0357  0.0715  0.1072  0.1429  0.1786  0.2144  0.2501  0.2858  0.3216   \n",
              "28  0.0644  0.1287  0.1931  0.2574  0.3218  0.3862  0.4505  0.5149  0.5792   \n",
              "29  0.0299  0.0598  0.0897  0.1196  0.1495  0.1794  0.2093  0.2392  0.2691   \n",
              "30  0.0307  0.0614  0.0921  0.1228  0.1535  0.1842  0.2149  0.2456  0.2763   \n",
              "31  0.0658  0.1316  0.1974  0.2632  0.3290  0.3948  0.4606  0.5264  0.5922   \n",
              "32  0.1626  0.3252  0.4879  0.6505  0.8131  0.9757  1.1383  1.3010  1.4636   \n",
              "33  1.0036  2.0072  3.0109  4.0145  5.0181  6.0217  7.0254  8.0290  9.0326   \n",
              "34  1.0055  2.0109  3.0164  4.0218  5.0273  6.0328  7.0382  8.0437  9.0492   \n",
              "35  1.0018  2.0035  3.0053  4.0070  5.0088  6.0105  7.0123  8.0140  9.0158   \n",
              "36  0.4191  0.8382  1.2573  1.6763  2.0954  2.5145  2.9336  3.3527  3.7718   \n",
              "37  0.5667  1.1335  1.7002  2.2670  2.8337  3.4005  3.9672  4.5340  5.1007   \n",
              "38  0.0940  0.1881  0.2821  0.3762  0.4702  0.5643  0.6583  0.7524  0.8464   \n",
              "39  0.0662  0.1325  0.1987  0.2649  0.3311  0.3974  0.4636  0.5298  0.5960   \n",
              "40  0.0757  0.1515  0.2272  0.3030  0.3787  0.4545  0.5302  0.6059  0.6817   \n",
              "41  0.0538  0.1076  0.1615  0.2153  0.2691  0.3229  0.3767  0.4305  0.4844   \n",
              "42  0.1715  0.3429  0.5144  0.6858  0.8573  1.0287  1.2002  1.3716  1.5431   \n",
              "43  0.1727  0.3455  0.5182  0.6909  0.8637  1.0364  1.2091  1.3819  1.5546   \n",
              "44  0.1854  0.3708  0.5562  0.7416  0.9269  1.1123  1.2977  1.4831  1.6685   \n",
              "45  0.3698  0.7395  1.1093  1.4791  1.8488  2.2186  2.5884  2.9581  3.3279   \n",
              "46  0.3841  0.7681  1.1522  1.5362  1.9203  2.3043  2.6884  3.0724  3.4565   \n",
              "47  0.1869  0.3738  0.5607  0.7476  0.9345  1.1214  1.3083  1.4952  1.6821   \n",
              "48  0.3240  0.6480  0.9721  1.2961  1.6201  1.9441  2.2681  2.5922  2.9162   \n",
              "49  0.2431  0.4862  0.7293  0.9724  1.2155  1.4586  1.7017  1.9448  2.1879   \n",
              "50     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
              "\n",
              "         9   ...      45      46      47      48      49      50      51  \\\n",
              "0    0.3387  ...  0.0080  0.0028  0.0008  0.0004  0.0005  0.0000  0.0004   \n",
              "1    0.1593  ...  0.0016  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "2    0.0640  ...  0.0090  0.0096  0.0048  0.0010  0.0016  0.0008  0.0012   \n",
              "3    0.0821  ...  0.0008  0.0034  0.0036  0.0004  0.0012  0.0000  0.0004   \n",
              "4    0.0950  ...  0.0004  0.0009  0.0000  0.0000  0.0004  0.0001  0.0004   \n",
              "5    0.3271  ...  0.0015  0.0024  0.0016  0.0032  0.0000  0.0003  0.0004   \n",
              "6    0.4867  ...  0.0005  0.0008  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "7    0.2062  ...  0.0000  0.0000  0.0004  0.0004  0.0000  0.0000  0.0004   \n",
              "8    0.1483  ...  0.0020  0.0004  0.0004  0.0004  0.0000  0.0001  0.0004   \n",
              "9    0.2037  ...  0.0044  0.0000  0.0008  0.0008  0.0004  0.0000  0.0004   \n",
              "10   0.8882  ...  0.0012  0.0016  0.0016  0.0000  0.0004  0.0000  0.0004   \n",
              "11   0.5060  ...  0.0004  0.0008  0.0000  0.0000  0.0000  0.0003  0.0004   \n",
              "12   0.8751  ...  0.0000  0.0000  0.0000  0.0008  0.0012  0.0000  0.0008   \n",
              "13   0.2237  ...  0.0081  0.0084  0.0012  0.0000  0.0000  0.0004  0.0008   \n",
              "14   0.2316  ...  0.0008  0.0012  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "15   0.2078  ...  0.0004  0.0004  0.0000  0.0000  0.0000  0.0004  0.0004   \n",
              "16   0.2004  ...  0.0012  0.0004  0.0000  0.0008  0.0008  0.0008  0.0008   \n",
              "17   0.2077  ...  0.0560  0.0096  0.0140  0.0076  0.0000  0.0004  0.0012   \n",
              "18   0.4091  ...  0.0114  0.0116  0.0024  0.0052  0.0028  0.0004  0.0008   \n",
              "19   0.3402  ...  0.0008  0.0000  0.0004  0.0000  0.0000  0.0000  0.0004   \n",
              "20   0.0899  ...  0.0176  0.0064  0.0080  0.0012  0.0040  0.0004  0.0020   \n",
              "21   0.2111  ...  0.0060  0.0048  0.0004  0.0008  0.0001  0.0004  0.0004   \n",
              "22   0.1714  ...  0.0064  0.0024  0.0021  0.0024  0.0012  0.0004  0.0008   \n",
              "23   0.2969  ...  0.0004  0.0001  0.0004  0.0000  0.0000  0.0000  0.0004   \n",
              "24   1.1885  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0013  0.0020   \n",
              "25   0.3564  ...  0.0000  0.0013  0.0016  0.0295  0.0296  0.1331  0.0016   \n",
              "26   0.5366  ...  0.0005  0.0000  0.0008  0.0184  0.0300  0.0324  0.1152   \n",
              "27   0.3573  ...  0.0000  0.0000  0.0012  0.0404  0.0432  0.0388  0.0068   \n",
              "28   0.6436  ...  0.0000  0.0000  0.0012  0.0430  0.0388  0.0388  0.0068   \n",
              "29   0.2990  ...  0.0000  0.0000  0.0004  0.0400  0.0100  0.0720  0.0092   \n",
              "30   0.3070  ...  0.0004  0.0004  0.0396  0.0403  0.0370  0.0380  0.0064   \n",
              "31   0.6580  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0944   \n",
              "32   1.6262  ...  0.0004  0.0005  0.0032  0.0005  0.0000  0.0214  0.0216   \n",
              "33  10.0360  ...  0.0000  0.0000  0.0010  0.0010  0.0000  0.0000  0.0072   \n",
              "34  10.0550  ...  0.0010  0.0000  0.0000  0.0020  0.0000  0.0020  0.0041   \n",
              "35  10.0180  ...  0.0019  0.0019  0.0000  0.0000  0.0000  0.0010  0.0038   \n",
              "36   4.1909  ...  0.0006  0.0000  0.0000  0.0017  0.0000  0.0000  0.0017   \n",
              "37   5.6675  ...  0.0017  0.0000  0.0000  0.0000  0.0000  0.0016  0.0017   \n",
              "38   0.9405  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004  0.0020   \n",
              "39   0.6623  ...  0.0000  0.0000  0.0000  0.0004  0.0000  0.0298  0.0304   \n",
              "40   0.7574  ...  0.0000  0.0000  0.0000  0.0001  0.0000  0.0301  0.0304   \n",
              "41   0.5382  ...  0.0001  0.0000  0.0007  0.0004  0.0000  0.0004  0.0304   \n",
              "42   1.7146  ...  0.0007  0.0020  0.0021  0.0027  0.0001  0.0007  0.0538   \n",
              "43   1.7273  ...  0.0007  0.0020  0.0000  0.0027  0.0013  0.0532  0.0538   \n",
              "44   1.8539  ...  0.0007  0.0007  0.0000  0.0020  0.0000  0.0007  0.0545   \n",
              "45   3.6977  ...  0.0000  0.0000  0.0005  0.0010  0.0000  0.0023  0.0092   \n",
              "46   3.8405  ...  0.0000  0.0017  0.0017  0.0003  0.0000  0.0000  0.0034   \n",
              "47   1.8690  ...  0.0733  0.0000  0.0000  0.0000  0.0002  0.0000  0.0035   \n",
              "48   3.2402  ...  0.0000  0.0000  0.0000  0.0010  0.0000  0.0000  0.0029   \n",
              "49   2.4310  ...  0.0023  0.0012  0.0018  0.0192  0.0000  0.0009  0.0192   \n",
              "50      NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
              "\n",
              "        52         53   54  \n",
              "0   0.0458     0.7891  1.0  \n",
              "1   0.0451     0.8106  1.0  \n",
              "2   0.0459  8732.0000  1.0  \n",
              "3   0.0446     0.7188  1.0  \n",
              "4   0.0454     0.8626  1.0  \n",
              "5   0.0418     0.9692  1.0  \n",
              "6   0.0429     0.9495  1.0  \n",
              "7   0.0425     0.8598  1.0  \n",
              "8   0.0462     0.8890  1.0  \n",
              "9   0.0455     0.7850  1.0  \n",
              "10  0.1729     0.9815  1.0  \n",
              "11  0.0898     0.9714  1.0  \n",
              "12  0.1550     0.9829  1.0  \n",
              "13  0.0462     0.7446  1.0  \n",
              "14  0.0450     0.7813  1.0  \n",
              "15  0.0462     0.7134  1.0  \n",
              "16  0.0495     0.9248  1.0  \n",
              "17  0.0504     0.5776  1.0  \n",
              "18  0.0496     0.5919  1.0  \n",
              "19  0.0454     0.7065  1.0  \n",
              "20  0.0450     0.7215  1.0  \n",
              "21  0.0455     0.7425  1.0  \n",
              "22  0.0457     0.7370  1.0  \n",
              "23  0.0463     0.7260  1.0  \n",
              "24  0.1970     0.9917  0.0  \n",
              "25  0.1945     0.9859  0.0  \n",
              "26  0.1945     0.9884  0.0  \n",
              "27  0.1565     0.9881  0.0  \n",
              "28  0.1565     0.9857  0.0  \n",
              "29  0.1567     0.9835  0.0  \n",
              "30  0.1574     0.9905  0.0  \n",
              "31  0.1337     0.9854  0.0  \n",
              "32  1.5060     0.9852  0.0  \n",
              "33  1.7322     0.9846  0.0  \n",
              "34  1.6474     0.9865  0.0  \n",
              "35  1.5185     0.9956  0.0  \n",
              "36  0.8517     0.9909  0.0  \n",
              "37  0.5892     0.9912  0.0  \n",
              "38  0.1507     0.9732  0.0  \n",
              "39  0.1440     0.9798  0.0  \n",
              "40  0.1439     0.9831  0.0  \n",
              "41  0.1448     0.9893  0.0  \n",
              "42  0.6818     0.9870  0.0  \n",
              "43  0.6818     0.9880  0.0  \n",
              "44  0.6405     0.9950  0.0  \n",
              "45  0.3057     0.9772  0.0  \n",
              "46  0.3056     0.9537  0.0  \n",
              "47  0.4535     0.9815  0.0  \n",
              "48  0.5212     0.9891  0.0  \n",
              "49  0.2224     0.9862  0.0  \n",
              "50     NaN        NaN  NaN  \n",
              "\n",
              "[51 rows x 55 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XobHgYzfvpwJ",
        "outputId": "63c45b34-4605-4046-93b0-edb96dabd70b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6637745b-8316-4e03-8bb9-0afc62e5844e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0339</td>\n",
              "      <td>0.0677</td>\n",
              "      <td>0.1016</td>\n",
              "      <td>0.1355</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.2371</td>\n",
              "      <td>0.2710</td>\n",
              "      <td>0.3049</td>\n",
              "      <td>0.3387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>0.7891</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.0478</td>\n",
              "      <td>0.0637</td>\n",
              "      <td>0.0796</td>\n",
              "      <td>0.0956</td>\n",
              "      <td>0.1115</td>\n",
              "      <td>0.1274</td>\n",
              "      <td>0.1434</td>\n",
              "      <td>0.1593</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.8106</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0128</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.0256</td>\n",
              "      <td>0.0320</td>\n",
              "      <td>0.0384</td>\n",
              "      <td>0.0448</td>\n",
              "      <td>0.0512</td>\n",
              "      <td>0.0576</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0459</td>\n",
              "      <td>8732.0000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0246</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.0411</td>\n",
              "      <td>0.0493</td>\n",
              "      <td>0.0575</td>\n",
              "      <td>0.0657</td>\n",
              "      <td>0.0739</td>\n",
              "      <td>0.0821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0446</td>\n",
              "      <td>0.7188</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0190</td>\n",
              "      <td>0.0285</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>0.0475</td>\n",
              "      <td>0.0570</td>\n",
              "      <td>0.0665</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0855</td>\n",
              "      <td>0.0950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.8626</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0327</td>\n",
              "      <td>0.0654</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1308</td>\n",
              "      <td>0.1635</td>\n",
              "      <td>0.1963</td>\n",
              "      <td>0.2290</td>\n",
              "      <td>0.2617</td>\n",
              "      <td>0.2944</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0418</td>\n",
              "      <td>0.9692</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0487</td>\n",
              "      <td>0.0973</td>\n",
              "      <td>0.1460</td>\n",
              "      <td>0.1947</td>\n",
              "      <td>0.2433</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>0.3407</td>\n",
              "      <td>0.3893</td>\n",
              "      <td>0.4380</td>\n",
              "      <td>0.4867</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0429</td>\n",
              "      <td>0.9495</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0206</td>\n",
              "      <td>0.0412</td>\n",
              "      <td>0.0618</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>0.1031</td>\n",
              "      <td>0.1237</td>\n",
              "      <td>0.1443</td>\n",
              "      <td>0.1649</td>\n",
              "      <td>0.1855</td>\n",
              "      <td>0.2062</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0425</td>\n",
              "      <td>0.8598</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0297</td>\n",
              "      <td>0.0445</td>\n",
              "      <td>0.0593</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0890</td>\n",
              "      <td>0.1038</td>\n",
              "      <td>0.1187</td>\n",
              "      <td>0.1335</td>\n",
              "      <td>0.1483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.8890</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0204</td>\n",
              "      <td>0.0407</td>\n",
              "      <td>0.0611</td>\n",
              "      <td>0.0815</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1222</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.2037</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0455</td>\n",
              "      <td>0.7850</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0888</td>\n",
              "      <td>0.1776</td>\n",
              "      <td>0.2665</td>\n",
              "      <td>0.3553</td>\n",
              "      <td>0.4441</td>\n",
              "      <td>0.5329</td>\n",
              "      <td>0.6217</td>\n",
              "      <td>0.7106</td>\n",
              "      <td>0.7994</td>\n",
              "      <td>0.8882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.9815</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0506</td>\n",
              "      <td>0.1012</td>\n",
              "      <td>0.1518</td>\n",
              "      <td>0.2024</td>\n",
              "      <td>0.2530</td>\n",
              "      <td>0.3036</td>\n",
              "      <td>0.3542</td>\n",
              "      <td>0.4048</td>\n",
              "      <td>0.4554</td>\n",
              "      <td>0.5060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0898</td>\n",
              "      <td>0.9714</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0875</td>\n",
              "      <td>0.1750</td>\n",
              "      <td>0.2625</td>\n",
              "      <td>0.3500</td>\n",
              "      <td>0.4376</td>\n",
              "      <td>0.5251</td>\n",
              "      <td>0.6126</td>\n",
              "      <td>0.7001</td>\n",
              "      <td>0.7876</td>\n",
              "      <td>0.8751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.1550</td>\n",
              "      <td>0.9829</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0224</td>\n",
              "      <td>0.0447</td>\n",
              "      <td>0.0671</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.1119</td>\n",
              "      <td>0.1342</td>\n",
              "      <td>0.1566</td>\n",
              "      <td>0.1790</td>\n",
              "      <td>0.2014</td>\n",
              "      <td>0.2237</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.7446</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0463</td>\n",
              "      <td>0.0695</td>\n",
              "      <td>0.0926</td>\n",
              "      <td>0.1158</td>\n",
              "      <td>0.1389</td>\n",
              "      <td>0.1621</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>0.2084</td>\n",
              "      <td>0.2316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0450</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0208</td>\n",
              "      <td>0.0416</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0831</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>0.1455</td>\n",
              "      <td>0.1662</td>\n",
              "      <td>0.1870</td>\n",
              "      <td>0.2078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0401</td>\n",
              "      <td>0.0601</td>\n",
              "      <td>0.0802</td>\n",
              "      <td>0.1002</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1603</td>\n",
              "      <td>0.1803</td>\n",
              "      <td>0.2004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.9248</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0208</td>\n",
              "      <td>0.0415</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0831</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>0.1246</td>\n",
              "      <td>0.1454</td>\n",
              "      <td>0.1662</td>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.2077</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0560</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0504</td>\n",
              "      <td>0.5776</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0818</td>\n",
              "      <td>0.1227</td>\n",
              "      <td>0.1637</td>\n",
              "      <td>0.2046</td>\n",
              "      <td>0.2455</td>\n",
              "      <td>0.2864</td>\n",
              "      <td>0.3273</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.4091</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0114</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0496</td>\n",
              "      <td>0.5919</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0340</td>\n",
              "      <td>0.0680</td>\n",
              "      <td>0.1021</td>\n",
              "      <td>0.1361</td>\n",
              "      <td>0.1701</td>\n",
              "      <td>0.2041</td>\n",
              "      <td>0.2381</td>\n",
              "      <td>0.2722</td>\n",
              "      <td>0.3062</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.7065</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0450</td>\n",
              "      <td>0.0540</td>\n",
              "      <td>0.0629</td>\n",
              "      <td>0.0719</td>\n",
              "      <td>0.0809</td>\n",
              "      <td>0.0899</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0450</td>\n",
              "      <td>0.7215</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0211</td>\n",
              "      <td>0.0422</td>\n",
              "      <td>0.0633</td>\n",
              "      <td>0.0844</td>\n",
              "      <td>0.1055</td>\n",
              "      <td>0.1266</td>\n",
              "      <td>0.1477</td>\n",
              "      <td>0.1689</td>\n",
              "      <td>0.1900</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0455</td>\n",
              "      <td>0.7425</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0343</td>\n",
              "      <td>0.0514</td>\n",
              "      <td>0.0686</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>0.1028</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1371</td>\n",
              "      <td>0.1542</td>\n",
              "      <td>0.1714</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.7370</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0297</td>\n",
              "      <td>0.0594</td>\n",
              "      <td>0.0891</td>\n",
              "      <td>0.1188</td>\n",
              "      <td>0.1484</td>\n",
              "      <td>0.1781</td>\n",
              "      <td>0.2078</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.2672</td>\n",
              "      <td>0.2969</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0463</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.1189</td>\n",
              "      <td>0.2377</td>\n",
              "      <td>0.3566</td>\n",
              "      <td>0.4754</td>\n",
              "      <td>0.5943</td>\n",
              "      <td>0.7131</td>\n",
              "      <td>0.8320</td>\n",
              "      <td>0.9508</td>\n",
              "      <td>1.0697</td>\n",
              "      <td>1.1885</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.9917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0356</td>\n",
              "      <td>0.0713</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>0.1782</td>\n",
              "      <td>0.2138</td>\n",
              "      <td>0.2495</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>0.3208</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0295</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.1331</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.9859</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0537</td>\n",
              "      <td>0.1073</td>\n",
              "      <td>0.1610</td>\n",
              "      <td>0.2146</td>\n",
              "      <td>0.2683</td>\n",
              "      <td>0.3219</td>\n",
              "      <td>0.3756</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.4829</td>\n",
              "      <td>0.5366</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.0300</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.1152</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.9884</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0715</td>\n",
              "      <td>0.1072</td>\n",
              "      <td>0.1429</td>\n",
              "      <td>0.1786</td>\n",
              "      <td>0.2144</td>\n",
              "      <td>0.2501</td>\n",
              "      <td>0.2858</td>\n",
              "      <td>0.3216</td>\n",
              "      <td>0.3573</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0404</td>\n",
              "      <td>0.0432</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.1565</td>\n",
              "      <td>0.9881</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0644</td>\n",
              "      <td>0.1287</td>\n",
              "      <td>0.1931</td>\n",
              "      <td>0.2574</td>\n",
              "      <td>0.3218</td>\n",
              "      <td>0.3862</td>\n",
              "      <td>0.4505</td>\n",
              "      <td>0.5149</td>\n",
              "      <td>0.5792</td>\n",
              "      <td>0.6436</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0430</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.1565</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0299</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.0897</td>\n",
              "      <td>0.1196</td>\n",
              "      <td>0.1495</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.2093</td>\n",
              "      <td>0.2392</td>\n",
              "      <td>0.2691</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0400</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0720</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.1567</td>\n",
              "      <td>0.9835</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.0307</td>\n",
              "      <td>0.0614</td>\n",
              "      <td>0.0921</td>\n",
              "      <td>0.1228</td>\n",
              "      <td>0.1535</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.2149</td>\n",
              "      <td>0.2456</td>\n",
              "      <td>0.2763</td>\n",
              "      <td>0.3070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0396</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>0.0370</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.1574</td>\n",
              "      <td>0.9905</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.0658</td>\n",
              "      <td>0.1316</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.2632</td>\n",
              "      <td>0.3290</td>\n",
              "      <td>0.3948</td>\n",
              "      <td>0.4606</td>\n",
              "      <td>0.5264</td>\n",
              "      <td>0.5922</td>\n",
              "      <td>0.6580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0944</td>\n",
              "      <td>0.1337</td>\n",
              "      <td>0.9854</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.1626</td>\n",
              "      <td>0.3252</td>\n",
              "      <td>0.4879</td>\n",
              "      <td>0.6505</td>\n",
              "      <td>0.8131</td>\n",
              "      <td>0.9757</td>\n",
              "      <td>1.1383</td>\n",
              "      <td>1.3010</td>\n",
              "      <td>1.4636</td>\n",
              "      <td>1.6262</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0216</td>\n",
              "      <td>1.5060</td>\n",
              "      <td>0.9852</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.0036</td>\n",
              "      <td>2.0072</td>\n",
              "      <td>3.0109</td>\n",
              "      <td>4.0145</td>\n",
              "      <td>5.0181</td>\n",
              "      <td>6.0217</td>\n",
              "      <td>7.0254</td>\n",
              "      <td>8.0290</td>\n",
              "      <td>9.0326</td>\n",
              "      <td>10.0360</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>1.7322</td>\n",
              "      <td>0.9846</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.0055</td>\n",
              "      <td>2.0109</td>\n",
              "      <td>3.0164</td>\n",
              "      <td>4.0218</td>\n",
              "      <td>5.0273</td>\n",
              "      <td>6.0328</td>\n",
              "      <td>7.0382</td>\n",
              "      <td>8.0437</td>\n",
              "      <td>9.0492</td>\n",
              "      <td>10.0550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>1.6474</td>\n",
              "      <td>0.9865</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1.0018</td>\n",
              "      <td>2.0035</td>\n",
              "      <td>3.0053</td>\n",
              "      <td>4.0070</td>\n",
              "      <td>5.0088</td>\n",
              "      <td>6.0105</td>\n",
              "      <td>7.0123</td>\n",
              "      <td>8.0140</td>\n",
              "      <td>9.0158</td>\n",
              "      <td>10.0180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>1.5185</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.4191</td>\n",
              "      <td>0.8382</td>\n",
              "      <td>1.2573</td>\n",
              "      <td>1.6763</td>\n",
              "      <td>2.0954</td>\n",
              "      <td>2.5145</td>\n",
              "      <td>2.9336</td>\n",
              "      <td>3.3527</td>\n",
              "      <td>3.7718</td>\n",
              "      <td>4.1909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.8517</td>\n",
              "      <td>0.9909</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.5667</td>\n",
              "      <td>1.1335</td>\n",
              "      <td>1.7002</td>\n",
              "      <td>2.2670</td>\n",
              "      <td>2.8337</td>\n",
              "      <td>3.4005</td>\n",
              "      <td>3.9672</td>\n",
              "      <td>4.5340</td>\n",
              "      <td>5.1007</td>\n",
              "      <td>5.6675</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.5892</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.0940</td>\n",
              "      <td>0.1881</td>\n",
              "      <td>0.2821</td>\n",
              "      <td>0.3762</td>\n",
              "      <td>0.4702</td>\n",
              "      <td>0.5643</td>\n",
              "      <td>0.6583</td>\n",
              "      <td>0.7524</td>\n",
              "      <td>0.8464</td>\n",
              "      <td>0.9405</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.1507</td>\n",
              "      <td>0.9732</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0662</td>\n",
              "      <td>0.1325</td>\n",
              "      <td>0.1987</td>\n",
              "      <td>0.2649</td>\n",
              "      <td>0.3311</td>\n",
              "      <td>0.3974</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.5298</td>\n",
              "      <td>0.5960</td>\n",
              "      <td>0.6623</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.9798</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.0757</td>\n",
              "      <td>0.1515</td>\n",
              "      <td>0.2272</td>\n",
              "      <td>0.3030</td>\n",
              "      <td>0.3787</td>\n",
              "      <td>0.4545</td>\n",
              "      <td>0.5302</td>\n",
              "      <td>0.6059</td>\n",
              "      <td>0.6817</td>\n",
              "      <td>0.7574</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0301</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.9831</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.1076</td>\n",
              "      <td>0.1615</td>\n",
              "      <td>0.2153</td>\n",
              "      <td>0.2691</td>\n",
              "      <td>0.3229</td>\n",
              "      <td>0.3767</td>\n",
              "      <td>0.4305</td>\n",
              "      <td>0.4844</td>\n",
              "      <td>0.5382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.1448</td>\n",
              "      <td>0.9893</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.3429</td>\n",
              "      <td>0.5144</td>\n",
              "      <td>0.6858</td>\n",
              "      <td>0.8573</td>\n",
              "      <td>1.0287</td>\n",
              "      <td>1.2002</td>\n",
              "      <td>1.3716</td>\n",
              "      <td>1.5431</td>\n",
              "      <td>1.7146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.6818</td>\n",
              "      <td>0.9870</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.1727</td>\n",
              "      <td>0.3455</td>\n",
              "      <td>0.5182</td>\n",
              "      <td>0.6909</td>\n",
              "      <td>0.8637</td>\n",
              "      <td>1.0364</td>\n",
              "      <td>1.2091</td>\n",
              "      <td>1.3819</td>\n",
              "      <td>1.5546</td>\n",
              "      <td>1.7273</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0532</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.6818</td>\n",
              "      <td>0.9880</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.1854</td>\n",
              "      <td>0.3708</td>\n",
              "      <td>0.5562</td>\n",
              "      <td>0.7416</td>\n",
              "      <td>0.9269</td>\n",
              "      <td>1.1123</td>\n",
              "      <td>1.2977</td>\n",
              "      <td>1.4831</td>\n",
              "      <td>1.6685</td>\n",
              "      <td>1.8539</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0545</td>\n",
              "      <td>0.6405</td>\n",
              "      <td>0.9950</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.3698</td>\n",
              "      <td>0.7395</td>\n",
              "      <td>1.1093</td>\n",
              "      <td>1.4791</td>\n",
              "      <td>1.8488</td>\n",
              "      <td>2.2186</td>\n",
              "      <td>2.5884</td>\n",
              "      <td>2.9581</td>\n",
              "      <td>3.3279</td>\n",
              "      <td>3.6977</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.3057</td>\n",
              "      <td>0.9772</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.3841</td>\n",
              "      <td>0.7681</td>\n",
              "      <td>1.1522</td>\n",
              "      <td>1.5362</td>\n",
              "      <td>1.9203</td>\n",
              "      <td>2.3043</td>\n",
              "      <td>2.6884</td>\n",
              "      <td>3.0724</td>\n",
              "      <td>3.4565</td>\n",
              "      <td>3.8405</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.3056</td>\n",
              "      <td>0.9537</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.3738</td>\n",
              "      <td>0.5607</td>\n",
              "      <td>0.7476</td>\n",
              "      <td>0.9345</td>\n",
              "      <td>1.1214</td>\n",
              "      <td>1.3083</td>\n",
              "      <td>1.4952</td>\n",
              "      <td>1.6821</td>\n",
              "      <td>1.8690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0733</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.4535</td>\n",
              "      <td>0.9815</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.3240</td>\n",
              "      <td>0.6480</td>\n",
              "      <td>0.9721</td>\n",
              "      <td>1.2961</td>\n",
              "      <td>1.6201</td>\n",
              "      <td>1.9441</td>\n",
              "      <td>2.2681</td>\n",
              "      <td>2.5922</td>\n",
              "      <td>2.9162</td>\n",
              "      <td>3.2402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.9891</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.4862</td>\n",
              "      <td>0.7293</td>\n",
              "      <td>0.9724</td>\n",
              "      <td>1.2155</td>\n",
              "      <td>1.4586</td>\n",
              "      <td>1.7017</td>\n",
              "      <td>1.9448</td>\n",
              "      <td>2.1879</td>\n",
              "      <td>2.4310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.2224</td>\n",
              "      <td>0.9862</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50 rows × 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6637745b-8316-4e03-8bb9-0afc62e5844e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6637745b-8316-4e03-8bb9-0afc62e5844e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6637745b-8316-4e03-8bb9-0afc62e5844e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        0       1       2       3       4       5       6       7       8   \\\n",
              "0   0.0339  0.0677  0.1016  0.1355  0.1694  0.2032  0.2371  0.2710  0.3049   \n",
              "1   0.0159  0.0319  0.0478  0.0637  0.0796  0.0956  0.1115  0.1274  0.1434   \n",
              "2   0.0064  0.0128  0.0192  0.0256  0.0320  0.0384  0.0448  0.0512  0.0576   \n",
              "3   0.0082  0.0164  0.0246  0.0328  0.0411  0.0493  0.0575  0.0657  0.0739   \n",
              "4   0.0095  0.0190  0.0285  0.0380  0.0475  0.0570  0.0665  0.0760  0.0855   \n",
              "5   0.0327  0.0654  0.0981  0.1308  0.1635  0.1963  0.2290  0.2617  0.2944   \n",
              "6   0.0487  0.0973  0.1460  0.1947  0.2433  0.2920  0.3407  0.3893  0.4380   \n",
              "7   0.0206  0.0412  0.0618  0.0825  0.1031  0.1237  0.1443  0.1649  0.1855   \n",
              "8   0.0148  0.0297  0.0445  0.0593  0.0742  0.0890  0.1038  0.1187  0.1335   \n",
              "9   0.0204  0.0407  0.0611  0.0815  0.1018  0.1222  0.1426  0.1630  0.1833   \n",
              "10  0.0888  0.1776  0.2665  0.3553  0.4441  0.5329  0.6217  0.7106  0.7994   \n",
              "11  0.0506  0.1012  0.1518  0.2024  0.2530  0.3036  0.3542  0.4048  0.4554   \n",
              "12  0.0875  0.1750  0.2625  0.3500  0.4376  0.5251  0.6126  0.7001  0.7876   \n",
              "13  0.0224  0.0447  0.0671  0.0895  0.1119  0.1342  0.1566  0.1790  0.2014   \n",
              "14  0.0232  0.0463  0.0695  0.0926  0.1158  0.1389  0.1621  0.1853  0.2084   \n",
              "15  0.0208  0.0416  0.0623  0.0831  0.1039  0.1247  0.1455  0.1662  0.1870   \n",
              "16  0.0200  0.0401  0.0601  0.0802  0.1002  0.1202  0.1403  0.1603  0.1803   \n",
              "17  0.0208  0.0415  0.0623  0.0831  0.1039  0.1246  0.1454  0.1662  0.1869   \n",
              "18  0.0409  0.0818  0.1227  0.1637  0.2046  0.2455  0.2864  0.3273  0.3682   \n",
              "19  0.0340  0.0680  0.1021  0.1361  0.1701  0.2041  0.2381  0.2722  0.3062   \n",
              "20  0.0090  0.0180  0.0270  0.0360  0.0450  0.0540  0.0629  0.0719  0.0809   \n",
              "21  0.0211  0.0422  0.0633  0.0844  0.1055  0.1266  0.1477  0.1689  0.1900   \n",
              "22  0.0171  0.0343  0.0514  0.0686  0.0857  0.1028  0.1200  0.1371  0.1542   \n",
              "23  0.0297  0.0594  0.0891  0.1188  0.1484  0.1781  0.2078  0.2375  0.2672   \n",
              "24  0.1189  0.2377  0.3566  0.4754  0.5943  0.7131  0.8320  0.9508  1.0697   \n",
              "25  0.0356  0.0713  0.1069  0.1426  0.1782  0.2138  0.2495  0.2851  0.3208   \n",
              "26  0.0537  0.1073  0.1610  0.2146  0.2683  0.3219  0.3756  0.4293  0.4829   \n",
              "27  0.0357  0.0715  0.1072  0.1429  0.1786  0.2144  0.2501  0.2858  0.3216   \n",
              "28  0.0644  0.1287  0.1931  0.2574  0.3218  0.3862  0.4505  0.5149  0.5792   \n",
              "29  0.0299  0.0598  0.0897  0.1196  0.1495  0.1794  0.2093  0.2392  0.2691   \n",
              "30  0.0307  0.0614  0.0921  0.1228  0.1535  0.1842  0.2149  0.2456  0.2763   \n",
              "31  0.0658  0.1316  0.1974  0.2632  0.3290  0.3948  0.4606  0.5264  0.5922   \n",
              "32  0.1626  0.3252  0.4879  0.6505  0.8131  0.9757  1.1383  1.3010  1.4636   \n",
              "33  1.0036  2.0072  3.0109  4.0145  5.0181  6.0217  7.0254  8.0290  9.0326   \n",
              "34  1.0055  2.0109  3.0164  4.0218  5.0273  6.0328  7.0382  8.0437  9.0492   \n",
              "35  1.0018  2.0035  3.0053  4.0070  5.0088  6.0105  7.0123  8.0140  9.0158   \n",
              "36  0.4191  0.8382  1.2573  1.6763  2.0954  2.5145  2.9336  3.3527  3.7718   \n",
              "37  0.5667  1.1335  1.7002  2.2670  2.8337  3.4005  3.9672  4.5340  5.1007   \n",
              "38  0.0940  0.1881  0.2821  0.3762  0.4702  0.5643  0.6583  0.7524  0.8464   \n",
              "39  0.0662  0.1325  0.1987  0.2649  0.3311  0.3974  0.4636  0.5298  0.5960   \n",
              "40  0.0757  0.1515  0.2272  0.3030  0.3787  0.4545  0.5302  0.6059  0.6817   \n",
              "41  0.0538  0.1076  0.1615  0.2153  0.2691  0.3229  0.3767  0.4305  0.4844   \n",
              "42  0.1715  0.3429  0.5144  0.6858  0.8573  1.0287  1.2002  1.3716  1.5431   \n",
              "43  0.1727  0.3455  0.5182  0.6909  0.8637  1.0364  1.2091  1.3819  1.5546   \n",
              "44  0.1854  0.3708  0.5562  0.7416  0.9269  1.1123  1.2977  1.4831  1.6685   \n",
              "45  0.3698  0.7395  1.1093  1.4791  1.8488  2.2186  2.5884  2.9581  3.3279   \n",
              "46  0.3841  0.7681  1.1522  1.5362  1.9203  2.3043  2.6884  3.0724  3.4565   \n",
              "47  0.1869  0.3738  0.5607  0.7476  0.9345  1.1214  1.3083  1.4952  1.6821   \n",
              "48  0.3240  0.6480  0.9721  1.2961  1.6201  1.9441  2.2681  2.5922  2.9162   \n",
              "49  0.2431  0.4862  0.7293  0.9724  1.2155  1.4586  1.7017  1.9448  2.1879   \n",
              "\n",
              "         9   ...      45      46      47      48      49      50      51  \\\n",
              "0    0.3387  ...  0.0080  0.0028  0.0008  0.0004  0.0005  0.0000  0.0004   \n",
              "1    0.1593  ...  0.0016  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "2    0.0640  ...  0.0090  0.0096  0.0048  0.0010  0.0016  0.0008  0.0012   \n",
              "3    0.0821  ...  0.0008  0.0034  0.0036  0.0004  0.0012  0.0000  0.0004   \n",
              "4    0.0950  ...  0.0004  0.0009  0.0000  0.0000  0.0004  0.0001  0.0004   \n",
              "5    0.3271  ...  0.0015  0.0024  0.0016  0.0032  0.0000  0.0003  0.0004   \n",
              "6    0.4867  ...  0.0005  0.0008  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "7    0.2062  ...  0.0000  0.0000  0.0004  0.0004  0.0000  0.0000  0.0004   \n",
              "8    0.1483  ...  0.0020  0.0004  0.0004  0.0004  0.0000  0.0001  0.0004   \n",
              "9    0.2037  ...  0.0044  0.0000  0.0008  0.0008  0.0004  0.0000  0.0004   \n",
              "10   0.8882  ...  0.0012  0.0016  0.0016  0.0000  0.0004  0.0000  0.0004   \n",
              "11   0.5060  ...  0.0004  0.0008  0.0000  0.0000  0.0000  0.0003  0.0004   \n",
              "12   0.8751  ...  0.0000  0.0000  0.0000  0.0008  0.0012  0.0000  0.0008   \n",
              "13   0.2237  ...  0.0081  0.0084  0.0012  0.0000  0.0000  0.0004  0.0008   \n",
              "14   0.2316  ...  0.0008  0.0012  0.0000  0.0000  0.0000  0.0000  0.0004   \n",
              "15   0.2078  ...  0.0004  0.0004  0.0000  0.0000  0.0000  0.0004  0.0004   \n",
              "16   0.2004  ...  0.0012  0.0004  0.0000  0.0008  0.0008  0.0008  0.0008   \n",
              "17   0.2077  ...  0.0560  0.0096  0.0140  0.0076  0.0000  0.0004  0.0012   \n",
              "18   0.4091  ...  0.0114  0.0116  0.0024  0.0052  0.0028  0.0004  0.0008   \n",
              "19   0.3402  ...  0.0008  0.0000  0.0004  0.0000  0.0000  0.0000  0.0004   \n",
              "20   0.0899  ...  0.0176  0.0064  0.0080  0.0012  0.0040  0.0004  0.0020   \n",
              "21   0.2111  ...  0.0060  0.0048  0.0004  0.0008  0.0001  0.0004  0.0004   \n",
              "22   0.1714  ...  0.0064  0.0024  0.0021  0.0024  0.0012  0.0004  0.0008   \n",
              "23   0.2969  ...  0.0004  0.0001  0.0004  0.0000  0.0000  0.0000  0.0004   \n",
              "24   1.1885  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0013  0.0020   \n",
              "25   0.3564  ...  0.0000  0.0013  0.0016  0.0295  0.0296  0.1331  0.0016   \n",
              "26   0.5366  ...  0.0005  0.0000  0.0008  0.0184  0.0300  0.0324  0.1152   \n",
              "27   0.3573  ...  0.0000  0.0000  0.0012  0.0404  0.0432  0.0388  0.0068   \n",
              "28   0.6436  ...  0.0000  0.0000  0.0012  0.0430  0.0388  0.0388  0.0068   \n",
              "29   0.2990  ...  0.0000  0.0000  0.0004  0.0400  0.0100  0.0720  0.0092   \n",
              "30   0.3070  ...  0.0004  0.0004  0.0396  0.0403  0.0370  0.0380  0.0064   \n",
              "31   0.6580  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0944   \n",
              "32   1.6262  ...  0.0004  0.0005  0.0032  0.0005  0.0000  0.0214  0.0216   \n",
              "33  10.0360  ...  0.0000  0.0000  0.0010  0.0010  0.0000  0.0000  0.0072   \n",
              "34  10.0550  ...  0.0010  0.0000  0.0000  0.0020  0.0000  0.0020  0.0041   \n",
              "35  10.0180  ...  0.0019  0.0019  0.0000  0.0000  0.0000  0.0010  0.0038   \n",
              "36   4.1909  ...  0.0006  0.0000  0.0000  0.0017  0.0000  0.0000  0.0017   \n",
              "37   5.6675  ...  0.0017  0.0000  0.0000  0.0000  0.0000  0.0016  0.0017   \n",
              "38   0.9405  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.0004  0.0020   \n",
              "39   0.6623  ...  0.0000  0.0000  0.0000  0.0004  0.0000  0.0298  0.0304   \n",
              "40   0.7574  ...  0.0000  0.0000  0.0000  0.0001  0.0000  0.0301  0.0304   \n",
              "41   0.5382  ...  0.0001  0.0000  0.0007  0.0004  0.0000  0.0004  0.0304   \n",
              "42   1.7146  ...  0.0007  0.0020  0.0021  0.0027  0.0001  0.0007  0.0538   \n",
              "43   1.7273  ...  0.0007  0.0020  0.0000  0.0027  0.0013  0.0532  0.0538   \n",
              "44   1.8539  ...  0.0007  0.0007  0.0000  0.0020  0.0000  0.0007  0.0545   \n",
              "45   3.6977  ...  0.0000  0.0000  0.0005  0.0010  0.0000  0.0023  0.0092   \n",
              "46   3.8405  ...  0.0000  0.0017  0.0017  0.0003  0.0000  0.0000  0.0034   \n",
              "47   1.8690  ...  0.0733  0.0000  0.0000  0.0000  0.0002  0.0000  0.0035   \n",
              "48   3.2402  ...  0.0000  0.0000  0.0000  0.0010  0.0000  0.0000  0.0029   \n",
              "49   2.4310  ...  0.0023  0.0012  0.0018  0.0192  0.0000  0.0009  0.0192   \n",
              "\n",
              "        52         53   54  \n",
              "0   0.0458     0.7891  1.0  \n",
              "1   0.0451     0.8106  1.0  \n",
              "2   0.0459  8732.0000  1.0  \n",
              "3   0.0446     0.7188  1.0  \n",
              "4   0.0454     0.8626  1.0  \n",
              "5   0.0418     0.9692  1.0  \n",
              "6   0.0429     0.9495  1.0  \n",
              "7   0.0425     0.8598  1.0  \n",
              "8   0.0462     0.8890  1.0  \n",
              "9   0.0455     0.7850  1.0  \n",
              "10  0.1729     0.9815  1.0  \n",
              "11  0.0898     0.9714  1.0  \n",
              "12  0.1550     0.9829  1.0  \n",
              "13  0.0462     0.7446  1.0  \n",
              "14  0.0450     0.7813  1.0  \n",
              "15  0.0462     0.7134  1.0  \n",
              "16  0.0495     0.9248  1.0  \n",
              "17  0.0504     0.5776  1.0  \n",
              "18  0.0496     0.5919  1.0  \n",
              "19  0.0454     0.7065  1.0  \n",
              "20  0.0450     0.7215  1.0  \n",
              "21  0.0455     0.7425  1.0  \n",
              "22  0.0457     0.7370  1.0  \n",
              "23  0.0463     0.7260  1.0  \n",
              "24  0.1970     0.9917  0.0  \n",
              "25  0.1945     0.9859  0.0  \n",
              "26  0.1945     0.9884  0.0  \n",
              "27  0.1565     0.9881  0.0  \n",
              "28  0.1565     0.9857  0.0  \n",
              "29  0.1567     0.9835  0.0  \n",
              "30  0.1574     0.9905  0.0  \n",
              "31  0.1337     0.9854  0.0  \n",
              "32  1.5060     0.9852  0.0  \n",
              "33  1.7322     0.9846  0.0  \n",
              "34  1.6474     0.9865  0.0  \n",
              "35  1.5185     0.9956  0.0  \n",
              "36  0.8517     0.9909  0.0  \n",
              "37  0.5892     0.9912  0.0  \n",
              "38  0.1507     0.9732  0.0  \n",
              "39  0.1440     0.9798  0.0  \n",
              "40  0.1439     0.9831  0.0  \n",
              "41  0.1448     0.9893  0.0  \n",
              "42  0.6818     0.9870  0.0  \n",
              "43  0.6818     0.9880  0.0  \n",
              "44  0.6405     0.9950  0.0  \n",
              "45  0.3057     0.9772  0.0  \n",
              "46  0.3056     0.9537  0.0  \n",
              "47  0.4535     0.9815  0.0  \n",
              "48  0.5212     0.9891  0.0  \n",
              "49  0.2224     0.9862  0.0  \n",
              "\n",
              "[50 rows x 55 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[0:50]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au45V3vavuax"
      },
      "outputs": [],
      "source": [
        "y_actual = df.iloc[:,54].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vUIXbVovx0U",
        "outputId": "2e17291f-c376-499c-e80f-dfceb8331d1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_actual=y_actual[:50]\n",
        "y_actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8oXy9ETv0oJ"
      },
      "outputs": [],
      "source": [
        "X_test = df.iloc[:,0:26].to_numpy()\n",
        "Y_test = df.iloc[:,26:52].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKM0pocLv377",
        "outputId": "aa237d18-36b4-4ecb-c8b9-26047b375243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50, 26)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXxl-7lrv5-B",
        "outputId": "a476fc18-aee2-475b-a16d-7bb5863c1e6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50, 26)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGczx08Hv8Q3",
        "outputId": "ea4e029a-f7c2-482e-e382-4eee88dd5f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 20ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict([X_test, Y_test])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b98LueMv-WY",
        "outputId": "a831517f-824b-4018-8150-36d987a3660d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.25222106e-03, 9.98747766e-01],\n",
              "       [1.08765985e-03, 9.98912275e-01],\n",
              "       [4.80477203e-04, 9.99519527e-01],\n",
              "       [4.97673824e-03, 9.95023191e-01],\n",
              "       [5.02488052e-04, 9.99497473e-01],\n",
              "       [9.26975976e-04, 9.99073029e-01],\n",
              "       [5.88420266e-03, 9.94115770e-01],\n",
              "       [7.87980214e-04, 9.99212086e-01],\n",
              "       [5.28194360e-04, 9.99471724e-01],\n",
              "       [7.52285763e-04, 9.99247789e-01],\n",
              "       [3.09054673e-01, 6.90945268e-01],\n",
              "       [5.68064079e-02, 9.43193495e-01],\n",
              "       [6.16758704e-01, 3.83241206e-01],\n",
              "       [6.09532639e-04, 9.99390483e-01],\n",
              "       [6.99563825e-04, 9.99300480e-01],\n",
              "       [5.72281424e-04, 9.99427676e-01],\n",
              "       [1.82938168e-03, 9.98170555e-01],\n",
              "       [5.35132363e-04, 9.99464869e-01],\n",
              "       [4.91588260e-04, 9.99508321e-01],\n",
              "       [2.70869769e-02, 9.72912967e-01],\n",
              "       [4.48023609e-04, 9.99551952e-01],\n",
              "       [5.45860617e-04, 9.99454141e-01],\n",
              "       [1.62314577e-03, 9.98376966e-01],\n",
              "       [1.03124499e-03, 9.98968601e-01],\n",
              "       [9.84498918e-01, 1.55011304e-02],\n",
              "       [9.86975014e-01, 1.30249262e-02],\n",
              "       [9.91226137e-01, 8.77381489e-03],\n",
              "       [9.85648274e-01, 1.43517684e-02],\n",
              "       [9.77839589e-01, 2.21603885e-02],\n",
              "       [9.78350759e-01, 2.16493141e-02],\n",
              "       [9.84050691e-01, 1.59492772e-02],\n",
              "       [9.92600799e-01, 7.39916740e-03],\n",
              "       [9.89445150e-01, 1.05547905e-02],\n",
              "       [9.92352605e-01, 7.64730247e-03],\n",
              "       [9.92451251e-01, 7.54869496e-03],\n",
              "       [9.93003011e-01, 6.99696038e-03],\n",
              "       [9.92644608e-01, 7.35534541e-03],\n",
              "       [9.92321253e-01, 7.67869316e-03],\n",
              "       [9.82749641e-01, 1.72503013e-02],\n",
              "       [9.89834845e-01, 1.01651559e-02],\n",
              "       [9.78503406e-01, 2.14964151e-02],\n",
              "       [9.87664282e-01, 1.23356823e-02],\n",
              "       [9.92688358e-01, 7.31151784e-03],\n",
              "       [9.93106008e-01, 6.89399848e-03],\n",
              "       [9.92495179e-01, 7.50486087e-03],\n",
              "       [9.89001095e-01, 1.09988982e-02],\n",
              "       [9.85092223e-01, 1.49077922e-02],\n",
              "       [9.92007136e-01, 7.99286272e-03],\n",
              "       [9.92222965e-01, 7.77701754e-03],\n",
              "       [9.91378903e-01, 8.62116180e-03]], dtype=float32)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmZMRHw8wA_l"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvGe27DuvTCI"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bAEjBf0vqzt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsLwMy2IwECX",
        "outputId": "3bc0e1c0-8977-4281-c7b1-bca0b494aa02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.98\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_actual, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM0ja5W1wHVD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
